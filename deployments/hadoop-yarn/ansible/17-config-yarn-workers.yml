#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

- name: "Configure YARN workers"
  hosts: workers
  gather_facts: false
  vars_files:
    - config/ansible.yml
    - /tmp/ansible-vars.yml

  tasks:

    #
    # Documentation
    # https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/ClusterSetup.html
    #
    # ResourceManager is on the master node, and NodeManager is on worker nodes.
    #
    # Comments and defaults from yarn-default.xml
    # [fedora@master01] vi /opt/hadoop/share/doc/hadoop/hadoop-yarn/hadoop-yarn-common/yarn-default.xml
    #

    - name: "Configure [{{hdhome}}/etc/hadoop/yarn-site.xml]"
      become: true
      blockinfile:
        path:   "{{hdhome}}/etc/hadoop/yarn-site.xml"
        marker: "<!-- {mark} Ansible managed configuration for NodeManager -->"
        insertbefore: "</configuration>"
        block: |
            <!--+
                | Enable ACLs.
                | Defaults to false.
                +-->
            <property>
                <name>yarn.acl.enable</name>
                <value>false</value>
            </property>

            <!--+
                | Single hostname that can be set in place of setting all yarn.resourcemanager*address resources.
                | Results in default ports for ResourceManager components.
                +-->
            <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>{{hdhost}}</value>
            </property>


            <!--+
                | Maximum limit of memory to allocate to each container request at the Resource Manager.
                | https://stackoverflow.com/a/43827548
                | https://github.com/hortonworks/hdp-configuration-utils
                +-->
            <property>
                <name>yarn.scheduler.maximum-allocation-mb</name>
                <value>43008</value>
            </property>

            <!--+
                | Minimum limit of memory to allocate to each container request at the Resource Manager.
                | https://stackoverflow.com/a/43827548
                | https://github.com/hortonworks/hdp-configuration-utils
                +-->
            <property>
                <name>yarn.scheduler.minimum-allocation-mb</name>
                <value>14336</value>
            </property>

            <!--+
                | Maximum limit of memory to allocate to each container request at the Resource Manager.
                | Amount of physical memory, in MB, that can be allocated for containers.
                | It means the amount of memory YARN can utilize on this node and therefore this property should be lower than the total memory of that machine.
                | https://stackoverflow.com/a/43827548
                | 44*1024 = 45056
                | https://github.com/hortonworks/hdp-configuration-utils
                +-->
            <property>
                <name>yarn.nodemanager.resource.memory-mb</name>
                <value>43008</value>
            </property>

            <!--+
                | "the general recommendation is ... set it to be equal to the number of physical cores on the machine"
                | https://serverfault.com/q/896783
                +-->
            <property>
                <name>yarn.nodemanager.resource.cpu-vcores</name>
                <value>13</value>
            </property>

            <!--+
                | The default value for yarn.scheduler.maximum-allocation-vcores is set to twice the number of CPUs.
                | This oversubscription assumes that CPUs are not always running a thread, and hence assigning more cores enables maximum CPU utilization.
                | https://serverfault.com/a/908778
                +-->
            <property>
                <name>yarn.scheduler.maximum-allocation-vcores</name>
                <value>48</value>
            </property>

            <property>
                <name>yarn.scheduler.minimum-allocation-vcores</name>
                <value>1</value>
            </property>

            <!--+
                | Comma-separated list of paths on the local filesystem where intermediate data is written.
                | Multiple paths help spread disk i/o.
            <property>
                <name>yarn.nodemanager.local-dirs</name>
                <value/>
            </property>
                +-->

            <!--+
                | Comma-separated list of paths on the local filesystem where logs are written.
                | Multiple paths help spread disk i/o.
            <property>
                <name>yarn.nodemanager.log-dirs</name>
                <value/>
            </property>
                +-->

            <!--+
                | HDFS directory where the application logs are moved on application completion.
                | Need to set appropriate permissions.
                | Only applicable if log-aggregation is enabled.
            <property>
                <name>yarn.nodemanager.remote-app-log-dir</name>
                <value>/logs</value>
            </property>
                +-->

            <!--+
                | Suffix appended to the remote log dir.
                | Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam}.
                | Only applicable if log-aggregation is enabled.
            <property>
                <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
                <value>/logs</value>
            </property>
                +-->

            <!--+
                | A comma separated list of services where service name should only contain a-zA-Z0-9_ and can not start with numbers.
                | mapreduce_shuffle : Shuffle service that needs to be set for Map Reduce applications.
                +-->
            <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
            </property>

            <!--+
                | Environment properties to be inherited by containers from NodeManagers.
                |   JAVA_HOME,
                |   HADOOP_CONF_DIR,
                |   HADOOP_HDFS_HOME,
                |   HADOOP_YARN_HOME,
                |   HADOOP_MAPRED_HOME,
                |   HADOOP_COMMON_HOME,
                |   CLASSPATH_PREPEND_DISTCACHE
                |
            <property>
                <name>yarn.nodemanager.env-whitelist </name>
                <value/>
            </property>
                +-->

            <!--+
                | Whether physical memory limits will be enforced for containers.
                | default: true
            <property>
                <name>yarn.nodemanager.pmem-check-enabled</name>
                <value>false</value>
            </property>
                +-->

            <!--+
                | Whether virtual memory limits will be enforced for containers.
                | default: true
            <property>
                <name>yarn.nodemanager.vmem-check-enabled</name>
                <value>false</value>
            </property>
                +-->

            <!--+
                | The minimum fraction of number of disks to be healthy for the nodemanager to launch new containers.
                | This correspond to both yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.
                | i.e. If there are less number of healthy local-dirs (or log-dirs) available, then new containers will not be launched on this node.
                | default: 0.25
            <property>
                <name>yarn.nodemanager.disk-health-checker.min-healthy-disks</name>
                <value>0.0</value>
            </property>
                +-->

            <!--+
                | The maximum percentage of disk space utilization allowed after which a disk is marked as bad.
                | Values can range from 0.0 to 100.0. If the value is greater than or equal to 100, the nodemanager will check for full disk.
                | This applies to yarn.nodemanager.local-dirs and yarn.nodemanager.log-dirs.
                | default: 90.0
            <property>
                <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
                <value>80.0</value>
            </property>
                +-->

    # https://issues.apache.org/jira/browse/HADOOP-14978?focusedCommentId=16619981#comment-16619981
    - name: "Configure [{{hdhome}}/etc/hadoop/yarn-env.sh]"
      become: true
      blockinfile:
        path:   "{{hdhome}}/etc/hadoop/yarn-env.sh"
        marker: "# {mark} Ansible managed edit"
        insertafter: "# export YARN_NODEMANAGER_OPTS="
        block: |
            # https://issues.apache.org/jira/browse/HADOOP-14978?focusedCommentId=16619981#comment-16619981
            # export YARN_NODEMANAGER_OPTS="--add-modules javax.activation"
            # https://stackoverflow.com/a/48170409
            # export YARN_NODEMANAGER_USER=fedora




