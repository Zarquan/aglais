#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# Following Notes from zrq here:



# -----------------------------------------------------
# Create Clouds YAML file
#[user@desktop]

cat > "${HOME}/clouds.yaml" << EOF

clouds:


  gaia-test:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

  gaia-test-super:
    auth:
      auth_url: https://cumulus.openstack.hpc.cam.ac.uk:5000/v3
      application_credential_id:     '$(secret 'stv-gaia-test.CREDENTIAL_ID')'
      application_credential_secret: '$(secret 'stv-gaia-test.CREDENTIAL_SECRET')'
    region_name: "RegionOne"
    interface: "public"
    identity_api_version: 3
    auth_type: "v3applicationcredential"

EOF



# -----------------------------------------------------
# Create our project config file.
#[user@desktop]

    cat > "${HOME:?}/aglais.env" << 'EOF'

AGLAIS_REPO='git@github.com:stvoutsin/aglais.git'
AGLAIS_HOME="${PROJECTS_ROOT:?}/aglais"
AGLAIS_CODE="${AGLAIS_HOME:?}"
AGLAIS_CLOUD=gaia-test
AGLAIS_USER=stv

EOF




# -----------------------------------------------------
# Edit hosts.yml file 
#[user@desktop]

  source "${HOME}/aglais.settings"
  nano ${AGLAIS_CODE:?}/experiments/zrq/ansible/hosts.yml
	..	
	keypair: ''
	...


# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible" \
        atolmis/ansible-client:latest \
        bash

	# Success



# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    cd "${ANSIBLE_CODE:?}"



# -----------------------------------------------------
# Run the initial part of our deployment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
localhost                  : ok=25   changed=18   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
master01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker03                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker04                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker05                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker06                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker07                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker08                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Run the Hadoop part of our deployment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
localhost                  : ok=25   changed=18   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
master01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker03                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker04                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker05                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker06                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker07                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker08                   : ok=4    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   

# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '


	2020-09-09 19:46:10,729 INFO namenode.FSImageFormatProtobuf: Image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 of size 398 bytes saved in 0 seconds .
	2020-09-09 19:46:10,737 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
	2020-09-09 19:46:10,742 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
	2020-09-09 19:46:10,743 INFO namenode.NameNode: SHUTDOWN_MSG: 
	/************************************************************
	SHUTDOWN_MSG: Shutting down NameNode at master01/10.10.1.57
	************************************************************/



# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-dfs.sh
	'	


	Starting namenodes on [master01]
	Starting datanodes
	Starting secondary namenodes [aglais-20200909-master01.novalocal]
	aglais-20200909-master01.novalocal: Warning: Permanently added 'aglais-20200909-master01.novalocal,fe80::f816:3eff:fe8f:e9f4%eth0' (ECDSA) to the list of known hosts.


# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
	'

	Configured Capacity: 2199023255552 (2 TB)
	Present Capacity: 2190360985600 (1.99 TB)
	DFS Remaining: 2190360969216 (1.99 TB)
	DFS Used: 16384 (16 KB)
	DFS Used%: 0.00%
	Replicated Blocks:
		Under replicated blocks: 0
		Blocks with corrupt replicas: 0
		Missing blocks: 0
		Missing blocks (with replication factor 1): 0
		Low redundancy blocks with highest priority to recover: 0
		Pending deletion blocks: 0
	Erasure Coded Block Groups: 
		Low redundancy block groups: 0
		Block groups with corrupt internal blocks: 0
		Missing block groups: 0
		Low redundancy blocks with highest priority to recover: 0
		Pending deletion blocks: 0

	-------------------------------------------------
	Live datanodes (4):

	Name: 10.10.0.119:9866 (worker04)
	Hostname: worker04
	Decommission Status : Normal
	Configured Capacity: 549755813888 (512 GB)
	DFS Used: 4096 (4 KB)
	Non DFS Used: 17297408 (16.50 MB)
	DFS Remaining: 547590242304 (509.98 GB)
	DFS Used%: 0.00%
	DFS Remaining%: 99.61%
	Configured Cache Capacity: 0 (0 B)
	Cache Used: 0 (0 B)
	Cache Remaining: 0 (0 B)
	Cache Used%: 100.00%
	Cache Remaining%: 0.00%
	Xceivers: 1
	Last contact: Wed Sep 09 19:46:50 UTC 2020
	Last Block Report: Wed Sep 09 19:46:26 UTC 2020
	Num of Blocks: 0


	Name: 10.10.1.113:9866 (worker02)
	Hostname: worker02
	Decommission Status : Normal
	Configured Capacity: 549755813888 (512 GB)
	DFS Used: 4096 (4 KB)
	Non DFS Used: 17297408 (16.50 MB)
	DFS Remaining: 547590242304 (509.98 GB)
	DFS Used%: 0.00%
	DFS Remaining%: 99.61%
	Configured Cache Capacity: 0 (0 B)
	Cache Used: 0 (0 B)
	Cache Remaining: 0 (0 B)
	Cache Used%: 100.00%
	Cache Remaining%: 0.00%
	Xceivers: 1
	Last contact: Wed Sep 09 19:46:50 UTC 2020
	Last Block Report: Wed Sep 09 19:46:26 UTC 2020
	Num of Blocks: 0


	Name: 10.10.2.192:9866 (worker01)
	Hostname: worker01
	Decommission Status : Normal
	Configured Capacity: 549755813888 (512 GB)
	DFS Used: 4096 (4 KB)
	Non DFS Used: 17297408 (16.50 MB)
	DFS Remaining: 547590242304 (509.98 GB)
	DFS Used%: 0.00%
	DFS Remaining%: 99.61%
	Configured Cache Capacity: 0 (0 B)
	Cache Used: 0 (0 B)
	Cache Remaining: 0 (0 B)
	Cache Used%: 100.00%
	Cache Remaining%: 0.00%
	Xceivers: 1
	Last contact: Wed Sep 09 19:46:50 UTC 2020
	Last Block Report: Wed Sep 09 19:46:26 UTC 2020
	Num of Blocks: 0


	Name: 10.10.2.223:9866 (worker03)
	Hostname: worker03
	Decommission Status : Normal
	Configured Capacity: 549755813888 (512 GB)
	DFS Used: 4096 (4 KB)
	Non DFS Used: 17297408 (16.50 MB)
	DFS Remaining: 547590242304 (509.98 GB)
	DFS Used%: 0.00%
	DFS Remaining%: 99.61%
	Configured Cache Capacity: 0 (0 B)
	Cache Used: 0 (0 B)
	Cache Remaining: 0 (0 B)
	Cache Used%: 100.00%
	Cache Remaining%: 0.00%
	Xceivers: 1
	Last contact: Wed Sep 09 19:46:50 UTC 2020
	Last Block Report: Wed Sep 09 19:46:26 UTC 2020
	Num of Blocks: 0



# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

      > Starting resourcemanager
        Starting nodemanagers


# -----------------------------------------------------
# Install the Spark binaries.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "20-install-spark.yml"


	> PLAY RECAP **********************************************************************************************************************************************
 	  master01                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
 	  zeppelin                   : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Add the security rules for Spark.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "21-config-spark-security.yml"

      
	> PLAY RECAP **********************************************************************************************************************************************
	localhost                  : ok=6    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   



# -----------------------------------------------------
# Create our Spark configuration.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "22-config-spark-master.yml"

	> PLAY RECAP **********************************************************************************************************************************************
	gateway                    : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
	master01                   : ok=2    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0  

# -----------------------------------------------------
# Create our HDFS log directory.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfs -mkdir /spark-log
        '


# -----------------------------------------------------
# Run the SparkPi example from the Spark install instructtions.
# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#launching-spark-on-yarn
#[root@ansibler]

    ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10


       > ..
 
	2020-09-09 20:11:34,731 INFO yarn.Client: Application report for application_1599680883948_0001 (state: FINISHED)
	2020-09-09 20:11:34,732 INFO yarn.Client: 
		 client token: N/A
		 diagnostics: N/A
		 ApplicationMaster host: worker03
		 ApplicationMaster RPC port: 43513
		 queue: default
		 start time: 1599682277508
		 final status: SUCCEEDED
		 tracking URL: http://master01:8088/proxy/application_1599680883948_0001/
		 user: fedora
	2020-09-09 20:11:34,752 INFO util.ShutdownHookManager: Shutdown hook called
	2020-09-09 20:11:34,756 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-30283984-2687-4b6c-a967-78dd4c424fb9
	2020-09-09 20:11:34,761 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c482f3d9-ab95-4287-87a8-b5c92285704f




# -----------------------------------------------------
# Run the Zeppelin install.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-04.yml"
	


# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh zeppelin \
        '
        sudo /opt/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh start
        '


