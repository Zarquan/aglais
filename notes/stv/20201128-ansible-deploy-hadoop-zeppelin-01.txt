 
#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#

    Target:

        Run the Ansible deploy 

    Result:

        Fail

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler15 \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds-dev.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:latest \
        bash

# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"


	> ---- ----
	  Done

# -----------------------------------------------------
# Run the main Ansible deployment.
#[root@ansibler]

    /hadoop-yarn/bin/create-all.sh \
        "${cloudname:?}"


# Some failures..

..

TASK [Install Java] *****************************************************************************************************************************************************************************************
fatal: [master02]: FAILED! => {"changed": false, "msg": "Failed to synchronize cache for repo 'updates-modular'", "rc": 1, "results": []}
changed: [worker02]
changed: [worker03]
changed: [worker01]
changed: [zeppelin]
changed: [master01]


..

TASK [Allow any from masters to Zeppelin] *******************************************************************************************************************************************************************
changed: [localhost]

TASK [Allow any from workers to Zeppelin] *******************************************************************************************************************************************************************
changed: [localhost]

PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=8    changed=6    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
localhost                  : ok=74   changed=61   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
master01                   : ok=21   changed=13   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
master02                   : ok=4    changed=2    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
worker01                   : ok=21   changed=16   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
worker02                   : ok=21   changed=16   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
worker03                   : ok=21   changed=16   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=51   changed=46   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


...

TASK [Install the public key] *******************************************************************************************************************************************************************************
changed: [master01] => (item=master01)
fatal: [master01]: FAILED! => {"msg": "The task includes an option with an undefined variable. The error was: 'ansible.vars.hostvars.HostVarsVars object' has no attribute 'publickey'\n\nThe error appears to be in '/hadoop-yarn/ansible/12-config-ssh-access.yml': line 92, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n    - name: \"Install the public key\"\n      ^ here\n"}
changed: [worker01] => (item=master01)
changed: [worker03] => (item=master01)
changed: [worker02] => (item=master01)
fatal: [worker02]: FAILED! => {"msg": "The task includes an option with an undefined variable. The error was: 'ansible.vars.hostvars.HostVarsVars object' has no attribute 'publickey'\n\nThe error appears to be in '/hadoop-yarn/ansible/12-config-ssh-access.yml': line 92, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n    - name: \"Install the public key\"\n      ^ here\n"}
fatal: [worker03]: FAILED! => {"msg": "The task includes an option with an undefined variable. The error was: 'ansible.vars.hostvars.HostVarsVars object' has no attribute 'publickey'\n\nThe error appears to be in '/hadoop-yarn/ansible/12-config-ssh-access.yml': line 92, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n    - name: \"Install the public key\"\n      ^ here\n"}
fatal: [worker01]: FAILED! => {"msg": "The task includes an option with an undefined variable. The error was: 'ansible.vars.hostvars.HostVarsVars object' has no attribute 'publickey'\n\nThe error appears to be in '/hadoop-yarn/ansible/12-config-ssh-access.yml': line 92, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n    - name: \"Install the public key\"\n      ^ here\n"}


..






---- ---- ----
File [cephfs-mount.sh]
Path [/hadoop-yarn/bin]
---- ---- ----
Cloud name [gaia-prod]
Share name [aglais-gaia-dr2]
Mount path [/data/gaia/dr2]
Share mode [ro]
---- ---- ----

Target [gaia-prod][aglais-gaia-dr2]
Cloud gaia-prod was not found.
Found  []
/hadoop-yarn/bin/cephfs-mount.sh: line 78: shareid: parameter null or not set
/hadoop-yarn/bin/cephfs-mount.sh: line 97: locations: parameter null or not set
/hadoop-yarn/bin/cephfs-mount.sh: line 105: locations: parameter null or not set
----
Ceph path []
Ceph size []
----
/hadoop-yarn/bin/cephfs-mount.sh: line 132: shareid: parameter null or not set
/hadoop-yarn/bin/cephfs-mount.sh: line 134: accessrule: parameter null or not set
----
Ceph user []
Ceph key  []

/hadoop-yarn/bin/cephfs-mount.sh: line 158: cephuser: parameter null or not set
/hadoop-yarn/ansible /
usage: ansible-playbook [-h] [--version] [-v] [-k] [--private-key PRIVATE_KEY_FILE] [-u REMOTE_USER] [-c CONNECTION] [-T TIMEOUT] [--ssh-common-args SSH_COMMON_ARGS] [--sftp-extra-args SFTP_EXTRA_ARGS]
                        [--scp-extra-args SCP_EXTRA_ARGS] [--ssh-extra-args SSH_EXTRA_ARGS] [--force-handlers] [--flush-cache] [-b] [--become-method BECOME_METHOD] [--become-user BECOME_USER] [-K]
                        [-t TAGS] [--skip-tags SKIP_TAGS] [-C] [--syntax-check] [-D] [-i INVENTORY] [--list-hosts] [-l SUBSET] [-e EXTRA_VARS] [--vault-id VAULT_IDS]
                        [--ask-vault-pass | --vault-password-file VAULT_PASSWORD_FILES] [-f FORKS] [-M MODULE_PATH] [--list-tasks] [--list-tags] [--step] [--start-at-task START_AT_TASK]
                        playbook [playbook ...]

Runs Ansible playbooks, executing the defined tasks on the targeted hosts.

positional arguments:
  playbook              Playbook(s)

optional arguments:
  --ask-vault-pass      ask for vault password
  --flush-cache         clear the fact cache for every host in inventory
  --force-handlers      run handlers even if a task fails
  --list-hosts          outputs a list of matching hosts; does not execute anything else
  --list-tags           list all available tags
  --list-tasks          list all tasks that would be executed
  --skip-tags SKIP_TAGS
                        only run plays and tasks whose tags do not match these values
  --start-at-task START_AT_TASK
                        start the playbook at the task matching this name
  --step                one-step-at-a-time: confirm each task before running
  --syntax-check        perform a syntax check on the playbook, but do not execute it
  --vault-id VAULT_IDS  the vault identity to use
  --vault-password-file VAULT_PASSWORD_FILES
                        vault password file
  --version             show program's version number, config file location, configured module search path, module location, executable location and exit
  -C, --check           don't make any changes; instead, try to predict some of the changes that may occur
  -D, --diff            when changing (small) files and templates, show the differences in those files; works great with --check
  -M MODULE_PATH, --module-path MODULE_PATH
                        prepend colon-separated path(s) to module library (default=~/.ansible/plugins/modules:/usr/share/ansible/plugins/modules)
  -e EXTRA_VARS, --extra-vars EXTRA_VARS
                        set additional variables as key=value or YAML/JSON, if filename prepend with @
  -f FORKS, --forks FORKS
                        specify number of parallel processes to use (default=5)
  -h, --help            show this help message and exit
  -i INVENTORY, --inventory INVENTORY, --inventory-file INVENTORY
                        specify inventory host path or comma separated host list. --inventory-file is deprecated
  -l SUBSET, --limit SUBSET
                        further limit selected hosts to an additional pattern
  -t TAGS, --tags TAGS  only run plays and tasks tagged with these values
  -v, --verbose         verbose mode (-vvv for more, -vvvv to enable connection debugging)

Connection Options:
  control as whom and how to connect to hosts

  --private-key PRIVATE_KEY_FILE, --key-file PRIVATE_KEY_FILE
                        use this file to authenticate the connection
  --scp-extra-args SCP_EXTRA_ARGS
                        specify extra arguments to pass to scp only (e.g. -l)
  --sftp-extra-args SFTP_EXTRA_ARGS
                        specify extra arguments to pass to sftp only (e.g. -f, -l)
  --ssh-common-args SSH_COMMON_ARGS
                        specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand)
  --ssh-extra-args SSH_EXTRA_ARGS
                        specify extra arguments to pass to ssh only (e.g. -R)
  -T TIMEOUT, --timeout TIMEOUT
                        override the connection timeout in seconds (default=10)
  -c CONNECTION, --connection CONNECTION
                        connection type to use (default=smart)
  -k, --ask-pass        ask for connection password
  -u REMOTE_USER, --user REMOTE_USER
                        connect as this user (default=None)

Privilege Escalation Options:
  control how and which user you become as on target hosts

  --become-method BECOME_METHOD
                        privilege escalation method to use (default=sudo), use `ansible-doc -t become -l` to list valid choices.
  --become-user BECOME_USER
                        run operations as this user (default=root)
  -K, --ask-become-pass
                        ask for privilege escalation password
  -b, --become          run operations with become (does not imply password prompting)
ERROR! Invalid extra vars data supplied. '@/tmp/ceph-vars.yml' could not be made into a dictionary
/

---- ---- ----
File [cephfs-mount.sh]
Path [/hadoop-yarn/bin]
---- ---- ----
Cloud name [gaia-prod]
Share name [aglais-user-nch]
Mount path [/user/nch]
Share mode [rw]
---- ---- ----

Target [gaia-prod][aglais-user-nch]
Cloud gaia-prod was not found.
Found  []
/hadoop-yarn/bin/cephfs-mount.sh: line 78: shareid: parameter null or not set
/hadoop-yarn/bin/cephfs-mount.sh: line 97: locations: parameter null or not set
/hadoop-yarn/bin/cephfs-mount.sh: line 105: locations: parameter null or not set
----
Ceph path []
Ceph size []
----
/hadoop-yarn/bin/cephfs-mount.sh: line 132: shareid: parameter null or not set
/hadoop-yarn/bin/cephfs-mount.sh: line 134: accessrule: parameter null or not set
----
Ceph user []
Ceph key  []

/hadoop-yarn/bin/cephfs-mount.sh: line 158: cephuser: parameter null or not set
/hadoop-yarn/ansible /
usage: ansible-playbook [-h] [--version] [-v] [-k] [--private-key PRIVATE_KEY_FILE] [-u REMOTE_USER] [-c CONNECTION] [-T TIMEOUT] [--ssh-common-args SSH_COMMON_ARGS] [--sftp-extra-args SFTP_EXTRA_ARGS]
                        [--scp-extra-args SCP_EXTRA_ARGS] [--ssh-extra-args SSH_EXTRA_ARGS] [--force-handlers] [--flush-cache] [-b] [--become-method BECOME_METHOD] [--become-user BECOME_USER] [-K]
                        [-t TAGS] [--skip-tags SKIP_TAGS] [-C] [--syntax-check] [-D] [-i INVENTORY] [--list-hosts] [-l SUBSET] [-e EXTRA_VARS] [--vault-id VAULT_IDS]
                        [--ask-vault-pass | --vault-password-file VAULT_PASSWORD_FILES] [-f FORKS] [-M MODULE_PATH] [--list-tasks] [--list-tags] [--step] [--start-at-task START_AT_TASK]
                        playbook [playbook ...]

Runs Ansible playbooks, executing the defined tasks on the targeted hosts.

positional arguments:
  playbook              Playbook(s)

optional arguments:
  --ask-vault-pass      ask for vault password
  --flush-cache         clear the fact cache for every host in inventory
  --force-handlers      run handlers even if a task fails
  --list-hosts          outputs a list of matching hosts; does not execute anything else
  --list-tags           list all available tags
  --list-tasks          list all tasks that would be executed
  --skip-tags SKIP_TAGS
                        only run plays and tasks whose tags do not match these values
  --start-at-task START_AT_TASK
                        start the playbook at the task matching this name
  --step                one-step-at-a-time: confirm each task before running
  --syntax-check        perform a syntax check on the playbook, but do not execute it
  --vault-id VAULT_IDS  the vault identity to use
  --vault-password-file VAULT_PASSWORD_FILES
                        vault password file
  --version             show program's version number, config file location, configured module search path, module location, executable location and exit
  -C, --check           don't make any changes; instead, try to predict some of the changes that may occur
  -D, --diff            when changing (small) files and templates, show the differences in those files; works great with --check
  -M MODULE_PATH, --module-path MODULE_PATH
                        prepend colon-separated path(s) to module library (default=~/.ansible/plugins/modules:/usr/share/ansible/plugins/modules)
  -e EXTRA_VARS, --extra-vars EXTRA_VARS
                        set additional variables as key=value or YAML/JSON, if filename prepend with @
  -f FORKS, --forks FORKS
                        specify number of parallel processes to use (default=5)
  -h, --help            show this help message and exit
  -i INVENTORY, --inventory INVENTORY, --inventory-file INVENTORY
                        specify inventory host path or comma separated host list. --inventory-file is deprecated
  -l SUBSET, --limit SUBSET
                        further limit selected hosts to an additional pattern
  -t TAGS, --tags TAGS  only run plays and tasks tagged with these values
  -v, --verbose         verbose mode (-vvv for more, -vvvv to enable connection debugging)

Connection Options:
  control as whom and how to connect to hosts

  --private-key PRIVATE_KEY_FILE, --key-file PRIVATE_KEY_FILE
                        use this file to authenticate the connection
  --scp-extra-args SCP_EXTRA_ARGS
                        specify extra arguments to pass to scp only (e.g. -l)
  --sftp-extra-args SFTP_EXTRA_ARGS
                        specify extra arguments to pass to sftp only (e.g. -f, -l)
  --ssh-common-args SSH_COMMON_ARGS
                        specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand)
  --ssh-extra-args SSH_EXTRA_ARGS
                        specify extra arguments to pass to ssh only (e.g. -R)
  -T TIMEOUT, --timeout TIMEOUT
                        override the connection timeout in seconds (default=10)
  -c CONNECTION, --connection CONNECTION
                        connection type to use (default=smart)
  -k, --ask-pass        ask for connection password
  -u REMOTE_USER, --user REMOTE_USER
                        connect as this user (default=None)

Privilege Escalation Options:
  control how and which user you become as on target hosts

  --become-method BECOME_METHOD
                        privilege escalation method to use (default=sudo), use `ansible-doc -t become -l` to list valid choices.
  --become-user BECOME_USER
                        run operations as this user (default=root)
  -K, --ask-become-pass
                        ask for privilege escalation password
  -b, --become          run operations with become (does not imply password prompting)
ERROR! Invalid extra vars data supplied. '@/tmp/ceph-vars.yml' could not be made into a dictionary





# --------------------------------------------------------


# Try running the failed sections again..

# Run the yaml files one by one, starting from the 12

 i.e.
    ..
    ansible-playbook         --inventory "hosts.yml"         "28-install-zeppelin-requirements.yml"
    ansible-playbook         --inventory "hosts.yml"         "29-install-pip-libs.yml"


  ssh master01 \
        '
        cd "${SPARK_HOME:?}"

        spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master yarn \
            --deploy-mode cluster \
            --driver-memory 1g \
            --executor-memory 1g \
            --executor-cores 1 \
            examples/jars/spark-examples*.jar \
                10
        '
2020-11-28 15:46:57,293 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-28 15:46:57,798 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.0.216:8032
2020-11-28 15:46:57,915 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers
2020-11-28 15:46:57,992 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
2020-11-28 15:46:57,993 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
2020-11-28 15:46:57,993 INFO yarn.Client: Setting up container launch context for our AM
2020-11-28 15:46:57,995 INFO yarn.Client: Setting up the launch environment for our AM container
2020-11-28 15:46:58,003 INFO yarn.Client: Preparing resources for our AM container
2020-11-28 15:46:58,053 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
2020-11-28 15:46:59,919 INFO yarn.Client: Uploading resource file:/opt/spark-2.4.7-bin-hadoop2.7/local/spark-3ef2a098-a813-4770-8d9c-4bd9898b49c8/__spark_libs__7783012859123303308.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1606575332220_0001/__spark_libs__7783012859123303308.zip
2020-11-28 15:47:00,884 INFO yarn.Client: Uploading resource file:/opt/spark-2.4.7-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.7.jar -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1606575332220_0001/spark-examples_2.11-2.4.7.jar
2020-11-28 15:47:00,978 INFO yarn.Client: Uploading resource file:/opt/spark-2.4.7-bin-hadoop2.7/local/spark-3ef2a098-a813-4770-8d9c-4bd9898b49c8/__spark_conf__4928873632959062949.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1606575332220_0001/__spark_conf__.zip
2020-11-28 15:47:01,018 INFO spark.SecurityManager: Changing view acls to: fedora
2020-11-28 15:47:01,018 INFO spark.SecurityManager: Changing modify acls to: fedora
2020-11-28 15:47:01,019 INFO spark.SecurityManager: Changing view acls groups to: 
2020-11-28 15:47:01,019 INFO spark.SecurityManager: Changing modify acls groups to: 
2020-11-28 15:47:01,020 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fedora); groups with view permissions: Set(); users  with modify permissions: Set(fedora); groups with modify permissions: Set()
2020-11-28 15:47:01,865 INFO yarn.Client: Submitting application application_1606575332220_0001 to ResourceManager
2020-11-28 15:47:02,054 INFO impl.YarnClientImpl: Submitted application application_1606575332220_0001
2020-11-28 15:47:03,057 INFO yarn.Client: Application report for application_1606575332220_0001 (state: ACCEPTED)
2020-11-28 15:47:03,060 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: [Sat Nov 28 15:47:02 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1606578421964
	 final status: UNDEFINED
	 tracking URL: http://master01:8088/proxy/application_1606575332220_0001/
	 user: fedora
2020-11-28 15:47:04,062 INFO yarn.Client: Application report for application_1606575332220_0001 (state: ACCEPTED)
2020-11-28 15:47:05,064 INFO yarn.Client: Application report for application_1606575332220_0001 (state: ACCEPTED)
2020-11-28 15:47:06,067 INFO yarn.Client: Application report for application_1606575332220_0001 (state: ACCEPTED)
2020-11-28 15:47:07,071 INFO yarn.Client: Application report for application_1606575332220_0001 (state: ACCEPTED)
2020-11-28 15:47:08,075 INFO yarn.Client: Application report for application_1606575332220_0001 (state: ACCEPTED)
2020-11-28 15:47:09,078 INFO yarn.Client: Application report for application_1606575332220_0001 (state: RUNNING)
2020-11-28 15:47:09,078 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: master01
	 ApplicationMaster RPC port: 39341
	 queue: default
	 start time: 1606578421964
	 final status: UNDEFINED
	 tracking URL: http://master01:8088/proxy/application_1606575332220_0001/
	 user: fedora
2020-11-28 15:47:10,080 INFO yarn.Client: Application report for application_1606575332220_0001 (state: RUNNING)
2020-11-28 15:47:11,084 INFO yarn.Client: Application report for application_1606575332220_0001 (state: RUNNING)
2020-11-28 15:47:12,087 INFO yarn.Client: Application report for application_1606575332220_0001 (state: RUNNING)
2020-11-28 15:47:13,094 INFO yarn.Client: Application report for application_1606575332220_0001 (state: RUNNING)
2020-11-28 15:47:14,096 INFO yarn.Client: Application report for application_1606575332220_0001 (state: FINISHED)
2020-11-28 15:47:14,096 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: master01
	 ApplicationMaster RPC port: 39341
	 queue: default
	 start time: 1606578421964
	 final status: SUCCEEDED
	 tracking URL: http://master01:8088/proxy/application_1606575332220_0001/
	 user: fedora
2020-11-28 15:47:14,106 INFO util.ShutdownHookManager: Shutdown hook called
2020-11-28 15:47:14,107 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b70c3ad3-8503-4f9f-b57b-13451ed3e9a7
2020-11-28 15:47:14,119 INFO util.ShutdownHookManager: Deleting directory /opt/spark-2.4.7-bin-hadoop2.7/local/spark-3ef2a098-a813-4770-8d9c-4bd9898b49c8




# --------------------------------------------------------------------
# Delete everything and run create all in the same ansibler container
#[root@ansibler]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"


	> ---- ----
	  Done

# -----------------------------------------------------
# Run the main Ansible deployment.
#[root@ansibler]

    /hadoop-yarn/bin/create-all.sh \
        "${cloudname:?}"


# Ceph mount is failing

# Fix: Include gaia-prod entry in the clouds.yaml file

