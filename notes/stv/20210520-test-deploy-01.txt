#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    Target:

        Test the latest settings for the tiny-16 deployment

    Result:

        Work in progress
        Test pass for 500 tree RandomForest
        Test fail for 1,000 tree RandomForest
            Out of disc space


# -----------------------------------------------------
# Checkout the deployment branch.
#[user@desktop]

    source "${HOME:?}/aglais.env"
    pushd "${AGLAIS_CODE}"

            git checkout '20210428-zrq-spark-conf'

    popd


# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    AGLAIS_CLOUD=gaia-test

    docker run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/deployments:/deployments:ro,z" \
        atolmis/ansible-client:2020.12.02 \
        bash


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"


     >   real    3m17.378s
     >   user    1m8.978s
     >   sys     0m9.824s


# -----------------------------------------------------
# Create everything, using the tiny-16 config.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            'tiny-16'


TASK [Download and unpack the spark-2.4.7 tar gzip file] ****************************************************************************************************************************************************
task path: /deployments/hadoop-yarn/ansible/20-install-spark.yml:33
fatal: [zeppelin]: FAILED! => {"changed": false, "msg": "Failure downloading https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz, HTTP Error 404: Not Found"}
fatal: [master01]: FAILED! => {"changed": false, "msg": "Failure downloading https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz, HTTP Error 404: Not Found"}

PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=108  changed=79   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
master01                   : ok=61   changed=33   unreachable=0    failed=1    skipped=6    rescued=0    ignored=0   
monitor                    : ok=13   changed=8    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
worker01                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker02                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker03                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker04                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker05                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker06                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker07                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker08                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker09                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker10                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker11                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker12                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker13                   : ok=66   changed=35   unreachable=0    failed=0    skipped=5    rescued=0    ignored=1   
worker14                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker15                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
worker16                   : ok=66   changed=36   unreachable=0    failed=0    skipped=5    rescued=0    ignored=0   
zeppelin                   : ok=57   changed=37   unreachable=0    failed=1    skipped=4    rescued=0    ignored=0   



# Looks like Spark 2.4.7 No longer available at the Spark/Downloads Page?
# https://downloads.apache.org/spark/ only has 2.4.8, 3.0.2, 3.1.1

# Change Ansible scripts to use Apache archive url and try again..



# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    time \
        /deployments/openstack/bin/delete-all.sh \
            "${cloudname:?}"


     >  Done

# -----------------------------------------------------
# Create everything, using the tiny-16 config.
#[root@ansibler]

    time \
        /deployments/hadoop-yarn/bin/create-all.sh \
            "${cloudname:?}" \
            'tiny-16'

      > Done


# -----------------------------------------------------
# Setup integration with github
#[root@ansibler]


  ssh zeppelin \
        '
        export githubuser=user
        export githubpass=pass

        rm -rf /home/fedora/zeppelin-0.8.2-bin-all/notebook
	git clone https://${githubuser:?}:${githubpass:?}@github.com/wfau/aglais-notebooks.git /home/fedora/zeppelin-0.8.2-bin-all/notebook

	cat > "${HOME}/zeppelin-0.8.2-bin-all/notebook/.git/hooks/post-commit" << EOF
	#!/bin/sh
	git push 

	EOF

	chmod +x ${HOME}/zeppelin-0.8.2-bin-all/notebook/.git/hooks/post-commit
	/home/fedora/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh restart
	'

	> Cloning into '/home/fedora/zeppelin-0.8.2-bin-all/notebook'...
	  Zeppelin stop                                              [  OK  ]
	  Zeppelin start                                             [  OK  ]

	# Success



# -----------------------------------------------------
# Login via Firefox
#[user@desktop]

    firefox --new-window "http://128.232.227.137:8080/" &



# -----------------------------------------------------
# Run a set of known Notebooks for validation
# -----------------------------------------------------

    # Modify the notebooks to use the new path to the Gaia EDR3 data
    # data_store = "file://///data/gaia/GEDR3_4096/GEDR3_GAIASOURCE/"



    # Mean proper motions over the sky [Success]
    # ----------------------------------------------------------

 
    # AglaisPublicExamples/Set Up [Success]
    # ----------------------------------------------------------
 

    # AglaisPublicExamples/5d kinematic clustering [Success]
    # ----------------------------------------------------------
 

    # AglaisPublicExamples/Data Holdings [Failed]
    # ----------------------------------------------------------
         # Empty tables
         # Looks like the Dataframes have no rows?
 
	+---------+---+---+---+-----+-----+
	|source_id| ra|dec|  G|g_ps1|r_ps1|
	+---------+---+---+---+-----+-----+
	+---------+---+---+---+-----+-----+


    # AglaisPublicExamples/Source counts over the sky [Failed]
    # ----------------------------------------------------------
    
    # Set the resolution level and define the query
	+------+---+
	|hpx_id|  n|
	+------+---+
	+------+---+
    
    # Plot up the results
        > ValueError: values must all be positive<Figure size 1164.96x720 with 2 Axes>


 
    # AglaisPublicExamples/Mean proper motions over the sky [Failed]
    # ----------------------------------------------------------

    # Mean RA proper motion plot
      > ValueError: Axis limits cannot be NaN or Inf<Figure size 1164.96x720 with 2 Axes>


    # AglaisPublicExamples/Good astrometric solutions via ML Random Forrest classifier [Failed]
    # ----------------------------------------------------------
 
    # Raw catalogue with selected columns
       > 0 

    # Define the training samples

	Good training data size: 0 rows
	---------------------------------------------------------------------------
	ZeroDivisionError                         Traceback (most recent call last)
	<ipython-input-86-68b5ab9f24a2> in <module>
	      9 # bad training data: negative parallaxes: N.B. make a selection exactly the same size as the good training set based on size of smaller (good) data set and count of all available bads
	     10 maximal_bad_ast_count = spark.sql('SELECT source_id FROM raw_sources WHERE parallax < -8.0').count()
	---> 11 filter_factor = int(maximal_bad_ast_count / good_training_rows)
	     12 all_bad_training_df = spark.sql('SELECT 0 AS label, ' + features_select_string + ' FROM raw_sources WHERE  parallax < -8.0 AND MOD(random_index, %d) = 0'%(filter_factor) + ' ORDER BY random_index LIMIT %d'%(good_training_rows))
	     13 all_bad_training_data_count = all_bad_training_df.count()

	ZeroDivisionError: division by zero


