#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#


    Target:

        Test the bulk-data-loading notebook.

        Continuation from prev notes

            notes/zrq/20210308-02-live-deployment.txt

    Result:




# -----------------------------------------------------
# Create the target data directories.
#[user@zeppelin]

    ssh master01 \
        '
        hdfs dfs -mkdir /partitioned
        hdfs dfs -mkdir /partitioned/gaia
        hdfs dfs -mkdir /partitioned/gaia/edr3
        '

# -----------------------------------------------------
# Create the test notebook ..
# Based on example from Nigel and Enrique.
#[user@zeppelin]


    %pyspark

    # number of buckets for our platform
    NUM_BUCKETS = 2048

    # the following based on example code kindly supplied by Enrique Utrilla:

    # Save a dataframe to a set of bucketed parquet files, repartitioning beforehand and sorting by source UID within the buckets:
    def saveToBinnedParquet(df, outputParquetPath, name, mode = "error", nBuckets = NUM_BUCKETS):
        df = df.repartition(nBuckets, "source_id")
        df.write.format("parquet") \
                .mode(mode) \
                .bucketBy(nBuckets, "source_id") \
                .sortBy("source_id") \
                .option("path", outputParquetPath) \
                .saveAsTable(name)




    %pyspark
    import sys

    # 1%:
    #gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*11.csv')
    # 10%:
    gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*1.csv')
    # full monty:
    #gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*.csv')

    saveToBinnedParquet(
        gaia_source_df,
        'hdfs://master01:9000/partitioned/gaia/edr3',
        name = 'gaia_source_bucketed_by_source_id',
        mode = 'overwrite'
        )



    10% Took 10 min 3 sec. Last updated by zrq at March 08 2021, 7:13:36 PM.

    100% failed after 4+hrs


# -----------------------------------------------------
# Check the Zeppelin interpreter logs.
#[fedora@zeppelin]



    >   ....
    >   ....
    >    INFO [2021-03-08 19:39:26,787] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 2694.2 in stage 185.0 (TID 611682, worker06, executor 15, partition 2694, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:26,788] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4825.2 in stage 185.0 (TID 611657) in 7837 ms on worker06 (executor 15) (1152/11932)
    >    INFO [2021-03-08 19:39:27,216] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 2371.2 in stage 185.0 (TID 611683, worker03, executor 8, partition 2371, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:27,216] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4536.2 in stage 185.0 (TID 611668) in 4376 ms on worker03 (executor 8) (1153/11932)
    >    INFO [2021-03-08 19:39:27,389] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 2303.2 in stage 185.0 (TID 611684, worker07, executor 11, partition 2303, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:27,389] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 1319.2 in stage 185.0 (TID 611667) in 5184 ms on worker07 (executor 11) (1154/11932)
    >    INFO [2021-03-08 19:39:27,592] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 1066.2 in stage 185.0 (TID 611685, worker02, executor 9, partition 1066, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:27,592] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 942.2 in stage 185.0 (TID 611669) in 4583 ms on worker02 (executor 9) (1155/11932)
    >    INFO [2021-03-08 19:39:27,643] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 401.2 in stage 185.0 (TID 611686, worker02, executor 9, partition 401, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:27,643] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3198.2 in stage 185.0 (TID 611670) in 4283 ms on worker02 (executor 9) (1156/11932)
    >    INFO [2021-03-08 19:39:27,796] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 2751.2 in stage 185.0 (TID 611687, worker03, executor 8, partition 2751, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:27,796] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2135.2 in stage 185.0 (TID 611672) in 4278 ms on worker03 (executor 8) (1157/11932)
    >    INFO [2021-03-08 19:39:28,354] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Starting task 377.2 in stage 185.0 (TID 611688, worker07, executor 11, partition 377, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:28,354] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 800.2 in stage 185.0 (TID 611671) in 4841 ms on worker07 (executor 11) (1158/11932)
    >    INFO [2021-03-08 19:39:30,067] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 552.2 in stage 185.0 (TID 611689, worker07, executor 11, partition 552, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:30,068] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 4917.2 in stage 185.0 (TID 611673) in 4885 ms on worker07 (executor 11) (1159/11932)
    >    INFO [2021-03-08 19:39:30,115] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 1383.2 in stage 185.0 (TID 611690, worker02, executor 9, partition 1383, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:30,115] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3903.2 in stage 185.0 (TID 611675) in 4301 ms on worker02 (executor 9) (1160/11932)
    >    INFO [2021-03-08 19:39:30,244] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 434.2 in stage 185.0 (TID 611691, worker07, executor 11, partition 434, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:30,244] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2020.2 in stage 185.0 (TID 611674) in 4908 ms on worker07 (executor 11) (1161/11932)
    >    INFO [2021-03-08 19:39:30,664] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4647.2 in stage 185.0 (TID 611692, worker03, executor 8, partition 4647, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:30,665] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 4708.3 in stage 185.0 (TID 611677) in 4459 ms on worker03 (executor 8) (1162/11932)
    >    INFO [2021-03-08 19:39:30,795] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 151.2 in stage 185.0 (TID 611693, worker02, executor 9, partition 151, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:30,795] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2827.2 in stage 185.0 (TID 611676) in 4612 ms on worker02 (executor 9) (1163/11932)
    >    INFO [2021-03-08 19:39:30,863] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1137.2 in stage 185.0 (TID 611694, worker03, executor 8, partition 1137, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:30,863] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4029.3 in stage 185.0 (TID 611678) in 4261 ms on worker03 (executor 8) (1164/11932)
    >    INFO [2021-03-08 19:39:31,626] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 2493.4 in stage 185.0 (TID 611695, worker03, executor 8, partition 2493, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:31,626] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2371.2 in stage 185.0 (TID 611683) in 4410 ms on worker03 (executor 8) (1165/11932)
    >    INFO [2021-03-08 19:39:31,902] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 3316.3 in stage 185.0 (TID 611696, worker06, executor 15, partition 3316, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:31,902] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 1913.2 in stage 185.0 (TID 611697, worker06, executor 15, partition 1913, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:31,902] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3630.2 in stage 185.0 (TID 611679) in 5177 ms on worker06 (executor 15) (1166/11932)
    >    INFO [2021-03-08 19:39:31,902] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 4268.2 in stage 185.0 (TID 611680) in 5141 ms on worker06 (executor 15) (1167/11932)
    >    INFO [2021-03-08 19:39:31,970] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 1589.2 in stage 185.0 (TID 611698, worker06, executor 15, partition 1589, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:31,970] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2694.2 in stage 185.0 (TID 611682) in 5183 ms on worker06 (executor 15) (1168/11932)
    >    INFO [2021-03-08 19:39:32,032] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 1557.2 in stage 185.0 (TID 611699, worker06, executor 15, partition 1557, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:32,032] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4891.2 in stage 185.0 (TID 611681) in 5269 ms on worker06 (executor 15) (1169/11932)
    >    INFO [2021-03-08 19:39:32,056] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 475.2 in stage 185.0 (TID 611700, worker02, executor 9, partition 475, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:32,056] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 401.2 in stage 185.0 (TID 611686) in 4413 ms on worker02 (executor 9) (1170/11932)
    >    INFO [2021-03-08 19:39:32,158] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 1915.2 in stage 185.0 (TID 611701, worker02, executor 9, partition 1915, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:32,158] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1066.2 in stage 185.0 (TID 611685) in 4566 ms on worker02 (executor 9) (1171/11932)
    >    INFO [2021-03-08 19:39:32,196] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 4601.3 in stage 185.0 (TID 611702, worker07, executor 11, partition 4601, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:32,196] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2303.2 in stage 185.0 (TID 611684) in 4807 ms on worker07 (executor 11) (1172/11932)
    >    INFO [2021-03-08 19:39:32,531] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 938.2 in stage 185.0 (TID 611703, worker03, executor 8, partition 938, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:32,531] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 2751.2 in stage 185.0 (TID 611687) in 4735 ms on worker03 (executor 8) (1173/11932)
    >    INFO [2021-03-08 19:39:33,264] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 2761.2 in stage 185.0 (TID 611704, worker07, executor 11, partition 2761, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:33,264] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 377.2 in stage 185.0 (TID 611688) in 4910 ms on worker07 (executor 11) (1174/11932)
    >    INFO [2021-03-08 19:39:34,332] ({dispatcher-event-loop-11} Logging.scala[logInfo]:54) - Starting task 3332.2 in stage 185.0 (TID 611705, worker02, executor 9, partition 3332, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:34,332] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1383.2 in stage 185.0 (TID 611690) in 4218 ms on worker02 (executor 9) (1175/11932)
    >    INFO [2021-03-08 19:39:35,003] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 851.2 in stage 185.0 (TID 611706, worker03, executor 8, partition 851, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:35,003] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 4647.2 in stage 185.0 (TID 611692) in 4339 ms on worker03 (executor 8) (1176/11932)
    >    INFO [2021-03-08 19:39:35,054] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 415.3 in stage 185.0 (TID 611707, worker07, executor 11, partition 415, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:35,054] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 552.2 in stage 185.0 (TID 611689) in 4987 ms on worker07 (executor 11) (1177/11932)
    >    INFO [2021-03-08 19:39:35,107] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 4844.3 in stage 185.0 (TID 611708, worker02, executor 9, partition 4844, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:39:35,107] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 151.2 in stage 185.0 (TID 611693) in 4312 ms on worker02 (executor 9) (1178/11932)
    >    INFO [2021-03-08 19:39:35,152] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 1132.2 in stage 185.0 (TID 611709, worker07, executor 11, partition 1132, PROCESS_LOCAL, 8269 bytes)
    >   ....
    >    INFO [2021-03-08 19:41:14,642] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Starting task 157.1 in stage 185.0 (TID 612054, worker07, executor 11, partition 157, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:41:14,642] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 1260.2 in stage 185.0 (TID 612039) in 4754 ms on worker07 (executor 11) (1524/11932)
    >    INFO [2021-03-08 19:41:14,671] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 4078.1 in stage 185.0 (TID 612055, worker07, executor 11, partition 4078, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:41:14,671] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 4627.3 in stage 185.0 (TID 612038) in 4820 ms on worker07 (executor 11) (1525/11932)
    >    INFO [2021-03-08 19:41:15,070] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 1871.1 in stage 185.0 (TID 612056, worker02, executor 9, partition 1871, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:41:15,070] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 1371.2 in stage 185.0 (TID 612040) in 4499 ms on worker02 (executor 9) (1526/11932)
    >    INFO [2021-03-08 19:41:15,897] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Disabling executor 8.
    >    INFO [2021-03-08 19:41:15,897] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 8 (epoch 79)
    >    INFO [2021-03-08 19:41:15,897] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Trying to remove executor 8 from BlockManagerMaster.
    >    INFO [2021-03-08 19:41:15,897] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(8, worker03, 36297, None)
    >    INFO [2021-03-08 19:41:15,897] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 8 successfully in removeExecutor
    >    INFO [2021-03-08 19:41:15,897] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 8 (epoch 79)
    >    WARN [2021-03-08 19:41:15,903] ({dispatcher-event-loop-12} Logging.scala[logWarning]:66) - Requesting driver to remove executor 11 for reason Container marked as failed: container_1615204633239_0002_01_000019 on host: worker07. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    WARN [2021-03-08 19:41:15,904] ({dispatcher-event-loop-12} Logging.scala[logWarning]:66) - Requesting driver to remove executor 15 for reason Container marked as failed: container_1615204633239_0002_01_000023 on host: worker06. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >   ERROR [2021-03-08 19:41:15,904] ({dispatcher-event-loop-12} Logging.scala[logError]:70) - Lost executor 11 on worker07: Container marked as failed: container_1615204633239_0002_01_000019 on host: worker07. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    WARN [2021-03-08 19:41:15,904] ({dispatcher-event-loop-7} Logging.scala[logWarning]:66) - Requesting driver to remove executor 9 for reason Container marked as failed: container_1615204633239_0002_01_000014 on host: worker02. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    INFO [2021-03-08 19:41:15,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2740), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 347), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2630), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2401), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2378), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,904] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2676), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,905] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3775), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,905] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 663), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,905] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2226), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,905] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4657), so marking it as still running.
    >   ....
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3614), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2946), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,909] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Task 612055 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 542), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 1582), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 245), so marking it as still running.
    >   ....
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4971), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 1435), so marking it as still running.
    >    WARN [2021-03-08 19:41:15,909] ({dispatcher-event-loop-10} Logging.scala[logWarning]:66) - Requesting driver to remove executor 8 for reason Container marked as failed: container_1615204633239_0002_01_000013 on host: worker03. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    INFO [2021-03-08 19:41:15,909] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Task 612045 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
    >    INFO [2021-03-08 19:41:15,909] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 1023), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,910] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Task 612054 failed because while it was being computed, its executor exited for a reason unrelated to the task. Not counting this failure towards the maximum number of failures for the task.
    >    INFO [2021-03-08 19:41:15,910] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 1905), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,910] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3695), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,910] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2310), so marking it as still running.
    >   ....
    >    INFO [2021-03-08 19:41:15,982] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 5107), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,982] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3406), so marking it as still running.
    >    INFO [2021-03-08 19:41:15,982] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4838), so marking it as still running.
    >    INFO [2021-03-08 19:43:18,034] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.3.117:46678) with ID 16
    >    INFO [2021-03-08 19:43:18,035] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 3900.4 in stage 185.0 (TID 612057, worker02, executor 16, partition 3900, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,035] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 2194.2 in stage 185.0 (TID 612058, worker02, executor 16, partition 2194, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,035] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 1754.2 in stage 185.0 (TID 612059, worker02, executor 16, partition 1754, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,035] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 3055.2 in stage 185.0 (TID 612060, worker02, executor 16, partition 3055, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,131] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Registering block manager worker02:42479 with 6.8 GB RAM, BlockManagerId(16, worker02, 42479, None)
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.0.55:58852) with ID 17
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 4838.2 in stage 185.0 (TID 612061, worker03, executor 17, partition 4838, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 3406.2 in stage 185.0 (TID 612062, worker03, executor 17, partition 3406, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 5107.2 in stage 185.0 (TID 612063, worker03, executor 17, partition 5107, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,145] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 3641.3 in stage 185.0 (TID 612064, worker03, executor 17, partition 3641, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:18,252] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Registering block manager worker03:46317 with 6.8 GB RAM, BlockManagerId(17, worker03, 46317, None)
    >    INFO [2021-03-08 19:43:18,486] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_226_piece0 in memory on worker02:42479 (size: 11.9 KB, free: 6.8 GB)
    >    INFO [2021-03-08 19:43:18,659] ({dispatcher-event-loop-4} Logging.scala[logInfo]:54) - Added broadcast_226_piece0 in memory on worker03:46317 (size: 11.9 KB, free: 6.8 GB)
    >    INFO [2021-03-08 19:43:19,343] ({dispatcher-event-loop-5} Logging.scala[logInfo]:54) - Added broadcast_225_piece0 in memory on worker02:42479 (size: 25.5 KB, free: 6.8 GB)
    >    INFO [2021-03-08 19:43:19,478] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Added broadcast_225_piece0 in memory on worker03:46317 (size: 25.5 KB, free: 6.8 GB)
    >    INFO [2021-03-08 19:43:25,235] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 1755.2 in stage 185.0 (TID 612065, worker02, executor 16, partition 1755, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,236] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 4216.3 in stage 185.0 (TID 612066, worker02, executor 16, partition 4216, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,236] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 1754.2 in stage 185.0 (TID 612059) in 7201 ms on worker02 (executor 16) (1/11932)
    >    INFO [2021-03-08 19:43:25,236] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3900.4 in stage 185.0 (TID 612057) in 7201 ms on worker02 (executor 16) (2/11932)
    >    INFO [2021-03-08 19:43:25,272] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 4855.3 in stage 185.0 (TID 612067, worker03, executor 17, partition 4855, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,272] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 4272.3 in stage 185.0 (TID 612068, worker03, executor 17, partition 4272, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,273] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 5107.2 in stage 185.0 (TID 612063) in 7128 ms on worker03 (executor 17) (3/11932)
    >    INFO [2021-03-08 19:43:25,273] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 4838.2 in stage 185.0 (TID 612061) in 7128 ms on worker03 (executor 17) (4/11932)
    >    INFO [2021-03-08 19:43:25,276] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 927.2 in stage 185.0 (TID 612069, worker03, executor 17, partition 927, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,277] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 3641.3 in stage 185.0 (TID 612064) in 7132 ms on worker03 (executor 17) (5/11932)
    >    INFO [2021-03-08 19:43:25,298] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 3458.2 in stage 185.0 (TID 612070, worker02, executor 16, partition 3458, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,298] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2194.2 in stage 185.0 (TID 612058) in 7263 ms on worker02 (executor 16) (6/11932)
    >    INFO [2021-03-08 19:43:25,314] ({dispatcher-event-loop-3} Logging.scala[logInfo]:54) - Starting task 4074.3 in stage 185.0 (TID 612071, worker03, executor 17, partition 4074, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,314] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 3406.2 in stage 185.0 (TID 612062) in 7169 ms on worker03 (executor 17) (7/11932)
    >    INFO [2021-03-08 19:43:25,414] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 2130.3 in stage 185.0 (TID 612072, worker02, executor 16, partition 2130, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:43:25,414] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 3055.2 in stage 185.0 (TID 612060) in 7379 ms on worker02 (executor 16) (8/11932)
    >   ....
    >    INFO [2021-03-08 19:45:10,516] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 2952.3 in stage 185.0 (TID 612254) in 4228 ms on worker03 (executor 17) (198/11932)
    >    INFO [2021-03-08 19:45:10,928] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Starting task 2748.3 in stage 185.0 (TID 612263, worker03, executor 17, partition 2748, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:10,928] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 4718.3 in stage 185.0 (TID 612255) in 4315 ms on worker03 (executor 17) (199/11932)
    >    INFO [2021-03-08 19:45:11,967] ({dispatcher-event-loop-10} Logging.scala[logInfo]:54) - Starting task 4683.4 in stage 185.0 (TID 612264, worker02, executor 16, partition 4683, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:11,967] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1647.3 in stage 185.0 (TID 612256) in 4541 ms on worker02 (executor 16) (200/11932)
    >    INFO [2021-03-08 19:45:12,884] ({dispatcher-event-loop-2} Logging.scala[logInfo]:54) - Starting task 1891.3 in stage 185.0 (TID 612265, worker02, executor 16, partition 1891, PROCESS_LOCAL, 8269 bytes)
    >    WARN [2021-03-08 19:45:12,887] ({task-result-getter-1} Logging.scala[logWarning]:66) - Lost task 3563.3 in stage 185.0 (TID 612257, worker02, executor 16): java.io.IOException: No space left on device
    >   	at java.io.FileOutputStream.writeBytes(Native Method)
    >   	at java.io.FileOutputStream.write(FileOutputStream.java:326)
    >   ....
    >    INFO [2021-03-08 19:45:13,111] ({dispatcher-event-loop-8} Logging.scala[logInfo]:54) - Starting task 3563.4 in stage 185.0 (TID 612266, worker02, executor 16, partition 3563, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:13,111] ({task-result-getter-2} Logging.scala[logInfo]:54) - Lost task 490.2 in stage 185.0 (TID 612259) on worker02, executor 16: java.io.IOException (No space left on device) [duplicate 1]
    >    INFO [2021-03-08 19:45:13,276] ({dispatcher-event-loop-7} Logging.scala[logInfo]:54) - Starting task 490.3 in stage 185.0 (TID 612267, worker02, executor 16, partition 490, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:13,276] ({task-result-getter-3} Logging.scala[logInfo]:54) - Lost task 3005.3 in stage 185.0 (TID 612260) on worker02, executor 16: java.io.IOException (No space left on device) [duplicate 2]
    >    INFO [2021-03-08 19:45:13,595] ({dispatcher-event-loop-13} Logging.scala[logInfo]:54) - Starting task 3005.4 in stage 185.0 (TID 612268, worker03, executor 17, partition 3005, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:13,595] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 1678.3 in stage 185.0 (TID 612258) in 4300 ms on worker03 (executor 17) (201/11932)
    >    INFO [2021-03-08 19:45:14,528] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Starting task 696.3 in stage 185.0 (TID 612269, worker03, executor 17, partition 696, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:14,528] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 2307.3 in stage 185.0 (TID 612261) in 4561 ms on worker03 (executor 17) (202/11932)
    >    INFO [2021-03-08 19:45:14,762] ({dispatcher-event-loop-9} Logging.scala[logInfo]:54) - Starting task 2581.3 in stage 185.0 (TID 612270, worker03, executor 17, partition 2581, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:14,763] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 3025.2 in stage 185.0 (TID 612262) in 4247 ms on worker03 (executor 17) (203/11932)
    >    INFO [2021-03-08 19:45:15,456] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 4447.3 in stage 185.0 (TID 612271, worker03, executor 17, partition 4447, PROCESS_LOCAL, 8269 bytes)
    >    INFO [2021-03-08 19:45:15,456] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 2748.3 in stage 185.0 (TID 612263) in 4528 ms on worker03 (executor 17) (204/11932)
    >    INFO [2021-03-08 19:45:16,029] ({dispatcher-event-loop-12} Logging.scala[logInfo]:54) - Disabling executor 16.
    >    INFO [2021-03-08 19:45:16,029] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 16 (epoch 83)
    >    INFO [2021-03-08 19:45:16,029] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Trying to remove executor 16 from BlockManagerMaster.
    >    INFO [2021-03-08 19:45:16,029] ({dispatcher-event-loop-6} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(16, worker02, 42479, None)
    >    INFO [2021-03-08 19:45:16,029] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 16 successfully in removeExecutor
    >    INFO [2021-03-08 19:45:16,029] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 16 (epoch 83)
    >   ERROR [2021-03-08 19:45:16,033] ({dispatcher-event-loop-4} Logging.scala[logError]:70) - Lost executor 16 on worker02: Container marked as failed: container_1615204633239_0002_01_000025 on host: worker02. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    WARN [2021-03-08 19:45:16,033] ({dispatcher-event-loop-13} Logging.scala[logWarning]:66) - Requesting driver to remove executor 16 for reason Container marked as failed: container_1615204633239_0002_01_000025 on host: worker02. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    WARN [2021-03-08 19:45:16,034] ({dispatcher-event-loop-13} Logging.scala[logWarning]:66) - Requesting driver to remove executor 17 for reason Container marked as failed: container_1615204633239_0002_01_000026 on host: worker03. Exit status: -100. Diagnostics: Container released on a *lost* node.
    >    INFO [2021-03-08 19:45:16,034] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3552), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,034] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3879), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,034] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2651), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,034] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3267), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,034] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3458), so marking it as still running.
    >   
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 3700), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4423), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 1702), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 1210), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4515), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 2288), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Resubmitted ShuffleMapTask(185, 4855), so marking it as still running.
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Executor lost: 17 (epoch 84)
    >    INFO [2021-03-08 19:45:16,044] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Trying to remove executor 17 from BlockManagerMaster.
    >    INFO [2021-03-08 19:45:16,044] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removing block manager BlockManagerId(17, worker03, 46317, None)
    >    INFO [2021-03-08 19:45:16,044] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Removed 17 successfully in removeExecutor
    >    INFO [2021-03-08 19:45:16,045] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Shuffle files lost for executor: 17 (epoch 84)
    >    WARN [2021-03-08 19:45:16,144] ({rpc-server-4-6} TransportChannelHandler.java[exceptionCaught]:78) - Exception in connection from /10.10.0.55:58852
    >   java.io.IOException: Connection reset by peer
    >   	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
    >   	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
    >   	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
    >   	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
    >   	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:377)
    >   	at io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)
    >   	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133)
    >   	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)
    >   	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
    >   	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
    >   	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
    >   	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    >   	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    >   	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    >   	at java.lang.Thread.run(Thread.java:748)


# -----------------------------------------------------
# -----------------------------------------------------
# Check the logs on a worker.
#[fedora@worker07]

    pushd /var/hadoop/logs

        less hadoop-fedora-nodemanager-gaia-prod-20210308-worker07.novalocal.log

    >   ....
    >   2021-03-08 18:17:14,463 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 18:27:14,464 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 18:37:14,464 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 18:47:14,464 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 18:57:14,465 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:07:14,466 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:17:14,466 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:27:14,466 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   ....
    >   2021-03-08 19:31:14,237 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:31:14,238 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:31:14,238 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:31:16,220 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000003 transitioned from RUNNING to KILLING
    >   2021-03-08 19:31:16,220 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000003
    >   2021-03-08 19:31:16,295 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000003 is : 143
    >   2021-03-08 19:31:16,308 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000003. Ignoring.
    >   2021-03-08 19:31:16,308 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000003 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:31:16,309 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000003
    >   2021-03-08 19:31:16,309 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000003
    >   2021-03-08 19:31:16,310 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000003 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:31:16,310 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000003 from application application_1615204633239_0002
    >   2021-03-08 19:31:16,312 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000003
    >   2021-03-08 19:31:16,312 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:31:18,314 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000003]
    >   2021-03-08 19:33:14,231 INFO org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir passed disk check, adding to list of valid directories.
    >   2021-03-08 19:33:14,232 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) turned good: 1/1 local-dirs are good: /var/hadoop/temp/nm-local-dir; 1/1 log-dirs are good: /var/hadoop/logs/userlogs
    >   ....
    >   2021-03-08 19:33:17,363 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1615204633239_0002_000001 (auth:SIMPLE)
    >   2021-03-08 19:33:17,366 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1615204633239_0002_01_000019 by user fedora
    >   2021-03-08 19:33:17,368 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       IP=10.10.2.213  OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000019
    >   2021-03-08 19:33:17,368 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1615204633239_0002_01_000019 to application application_1615204633239_0002
    >   2021-03-08 19:33:17,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000019 transitioned from NEW to LOCALIZING
    >   2021-03-08 19:33:17,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1615204633239_0002
    >   2021-03-08 19:33:17,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000019 transitioned from LOCALIZING to SCHEDULED
    >   2021-03-08 19:33:17,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler: Starting container [container_1615204633239_0002_01_000019]
    >   2021-03-08 19:33:17,384 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000019 transitioned from SCHEDULED to RUNNING
    >   2021-03-08 19:33:17,384 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1615204633239_0002_01_000019
    >   2021-03-08 19:33:17,386 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000019/default_container_executor.sh]
    >   2021-03-08 19:33:18,890 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: container_1615204633239_0002_01_000019's ip = 10.10.1.91, and hostname = worker07
    >   2021-03-08 19:33:18,898 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Skipping monitoring container container_1615204633239_0002_01_000019 since CPU usage is not yet available.
    >   2021-03-08 19:37:14,466 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   ....
    >   2021-03-08 19:41:14,230 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:41:14,232 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:41:14,232 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:41:15,970 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000019 transitioned from RUNNING to KILLING
    >   2021-03-08 19:41:15,970 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000019
    >   2021-03-08 19:41:16,020 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000019 is : 143
    >   2021-03-08 19:41:16,031 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000019. Ignoring.
    >   2021-03-08 19:41:16,032 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000019 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:41:16,032 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000019
    >   2021-03-08 19:41:16,032 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000019
    >   2021-03-08 19:41:16,034 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000019 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:41:16,034 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000019 from application application_1615204633239_0002
    >   2021-03-08 19:41:16,034 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000019
    >   2021-03-08 19:41:16,034 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:41:17,036 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000019]
    >   2021-03-08 19:47:14,467 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:57:14,467 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:07:14,468 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   ....


# -----------------------------------------------------
# Check the available disc space.
#[fedora@worker07]

    ls -al /var/hadoop/temp

    >   lrwxrwxrwx. 1 root root 26 Mar  8 11:44 /var/hadoop/temp -> /mnt/local/vdb/hadoop/temp


    df -h /var/hadoop/temp/nm-local-dir

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdb         59G   51G  5.1G  91% /mnt/local/vdb


    du -h /var/hadoop/temp/

    >   ....
    >   ....
    >   26G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-0ae9fbe4-f7de-46ce-84c4-e0670f6de717
    >   51G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002
    >   51G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache
    >   51G     /var/hadoop/temp/nm-local-dir/usercache/fedora
    >   51G     /var/hadoop/temp/nm-local-dir/usercache
    >   51G     /var/hadoop/temp/nm-local-dir
    >   51G     /var/hadoop/temp/


# -----------------------------------------------------
# -----------------------------------------------------
# Check the logs on another worker.
#[fedora@worker05]

    pushd /var/hadoop/logs

        less hadoop-fedora-nodemanager-gaia-prod-20210308-worker05.novalocal.log

    >   ....
    >   ....
    >   2021-03-08 18:37:14,496 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 18:47:14,496 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 18:57:14,496 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:07:14,497 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:17:14,497 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:27:14,497 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:29:14,261 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:29:14,718 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir :
    >    used space above threshold of 90.0% ] ;
    >   2021-03-08 19:29:14,718 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-
    >   local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:29:15,614 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000006 transitioned from RUNNING to KILLING
    >   2021-03-08 19:29:15,614 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000006
    >   2021-03-08 19:29:15,672 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000006 is : 143
    >   2021-03-08 19:29:15,685 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000006. Ignoring.
    >   2021-03-08 19:29:15,685 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000006 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:29:15,686 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002
    >   _01_000006
    >   2021-03-08 19:29:15,686 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=c
    >   ontainer_1615204633239_0002_01_000006
    >   2021-03-08 19:29:15,686 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000006 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:29:15,686 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000006 from application application_1615204633239_0002
    >   2021-03-08 19:29:15,687 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000006
    >   2021-03-08 19:29:15,687 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:29:18,692 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000006]
    >   2021-03-08 19:31:14,256 INFO org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir passed disk check, adding to list of valid directories.
    >   2021-03-08 19:31:14,257 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) turned good: 1/1 local-dirs are good: /var/hadoop/temp/nm-local-dir; 1/1 log-dirs are good: /var/hadoop/logs/userlogs
    >   2021-03-08 19:31:18,286 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1615204633239_0002_000001 (auth:SIMPLE)
    >   2021-03-08 19:31:18,295 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1615204633239_0002_01_000015 by user fedora
    >   2021-03-08 19:31:18,296 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       IP=10.10.2.213  OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_16152046332
    >   39_0002    CONTAINERID=container_1615204633239_0002_01_000015
    >
    >   2021-03-08 19:31:18,298 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from NEW to LOCALIZING
    >   2021-03-08 19:31:18,298 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1615204633239_0002
    >   2021-03-08 19:31:18,298 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from LOCALIZING to SCHEDULED
    >   2021-03-08 19:31:18,298 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler: Starting container [container_1615204633239_0002_01_000015]
    >   2021-03-08 19:31:18,311 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from SCHEDULED to RUNNING
    >   2021-03-08 19:31:18,311 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1615204633239_0002_01_000015
    >   2021-03-08 19:31:18,313 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000015/default_container_executor.sh]
    >   2021-03-08 19:31:19,997 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: container_1615204633239_0002_01_000015's ip = 10.10.3.12, and hostname = worker05
    >   2021-03-08 19:31:20,005 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Skipping monitoring container container_1615204633239_0002_01_000015 since CPU usage is not yet available.
    >   2021-03-08 19:37:14,256 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:37:14,256 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:37:14,256 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:37:14,497 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:37:15,282 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from RUNNING to KILLING
    >   2021-03-08 19:37:15,283 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000015
    >   2021-03-08 19:37:15,324 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000015 is : 143
    >   2021-03-08 19:37:15,335 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000015. Ignoring.
    >   2021-03-08 19:37:15,336 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:37:15,336 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000015
    >   2021-03-08 19:37:15,336 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000015
    >   2021-03-08 19:37:15,336 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000015 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:37:15,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000015 from application application_1615204633239_0002
    >   2021-03-08 19:37:15,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000015
    >   2021-03-08 19:37:15,337 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:37:16,339 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000015]
    >   2021-03-08 19:47:14,498 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:57:14,499 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:07:14,498 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:17:14,499 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:27:14,499 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:37:14,500 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0


# Check the available disc space.
#[fedora@worker05]

    ls -al /var/hadoop/temp

    >   lrwxrwxrwx. 1 root root 26 Mar  8 11:44 /var/hadoop/temp -> /mnt/local/vdb/hadoop/temp


    df -h /var/hadoop/temp/nm-local-dir

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdb         59G   53G  3.9G  94% /mnt/local/vdb


    du -h /var/hadoop/temp/

    >   ....
    >   ....
    >   414M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-f65cd889-ebc8-4d40-a3f7-39a38d6eb5d2/0c
    >   761M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-f65cd889-ebc8-4d40-a3f7-39a38d6eb5d2/33
    >   35G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-f65cd889-ebc8-4d40-a3f7-39a38d6eb5d2
    >   52G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002
    >   52G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache
    >   52G     /var/hadoop/temp/nm-local-dir/usercache/fedora
    >   52G     /var/hadoop/temp/nm-local-dir/usercache
    >   52G     /var/hadoop/temp/nm-local-dir
    >   52G     /var/hadoop/temp/



# -----------------------------------------------------
# -----------------------------------------------------
# Check the logs on another worker.
#[fedora@worker03]

    pushd /var/hadoop/logs

        less hadoop-fedora-nodemanager-gaia-prod-20210308-worker03.novalocal.log

    >   ....
    >   ....
    >   2021-03-08 19:43:15,996 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from NEW to LOCALIZING
    >   2021-03-08 19:43:15,996 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1615204633239_0002
    >   2021-03-08 19:43:15,996 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from LOCALIZING to SCHEDULED
    >   2021-03-08 19:43:15,997 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler: Starting container [container_1615204633239_0002_01_000026]
    >   2021-03-08 19:43:16,010 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from SCHEDULED to RUNNING
    >   2021-03-08 19:43:16,010 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1615204633239_0002_01_000026
    >   2021-03-08 19:43:16,013 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000026/default_container_executor.sh]
    >   2021-03-08 19:43:16,589 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: container_1615204633239_0002_01_000026's ip = 10.10.0.55, and hostname = worker03
    >   2021-03-08 19:43:16,597 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Skipping monitoring container container_1615204633239_0002_01_000026 since CPU usage is not yet available.
    >   2021-03-08 19:45:14,139 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:45:14,139 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:45:14,139 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:45:15,543 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from RUNNING to KILLING
    >   2021-03-08 19:45:15,543 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000026
    >   2021-03-08 19:45:15,577 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000026 is : 143
    >   2021-03-08 19:45:15,591 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000026. Ignoring.
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000026
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000026
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000026 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:45:15,592 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000026 from application application_1615204633239_0002
    >   2021-03-08 19:45:15,593 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000026
    >   2021-03-08 19:45:15,593 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:45:16,595 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000026]
    >   2021-03-08 19:47:14,348 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:57:14,349 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:07:14,350 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:17:14,354 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:27:14,354 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0


# -----------------------------------------------------
# Check the available disc space.
#[fedora@worker03]

    ls -al /var/hadoop/temp

    >   lrwxrwxrwx. 1 root root 26 Mar  8 11:44 /var/hadoop/temp -> /mnt/local/vdb/hadoop/temp


    df -h /var/hadoop/temp/nm-local-dir

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdb         59G   51G  5.5G  91% /mnt/local/vdb


    du -h /var/hadoop/temp/

    >   ....
    >   ....
    >   555M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/38
    >   622M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/02
    >   690M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/03
    >   554M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/1c
    >   693M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/0f
    >   553M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe/08
    >   30G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-b59abf63-5994-42b8-8640-6ea8aa01dafe
    >   21M     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/spark-87942b10-5b09-4aa8-9bf3-68b100939d9c
    >   21M     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/spark-d9756900-68b6-4338-800e-3e817e17099c
    >   50G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002
    >   50G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache
    >   51G     /var/hadoop/temp/nm-local-dir/usercache/fedora
    >   51G     /var/hadoop/temp/nm-local-dir/usercache
    >   51G     /var/hadoop/temp/nm-local-dir
    >   51G     /var/hadoop/temp/



# -----------------------------------------------------
# -----------------------------------------------------
# Check the logs on another worker.
#[fedora@worker01]

    pushd /var/hadoop/logs

        less hadoop-fedora-nodemanager-gaia-prod-20210308-worker01.novalocal.log

    >   ....
    >   2021-03-08 19:31:18,910 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000002]
    >   2021-03-08 19:33:14,299 INFO org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir passed disk check, adding to list of valid directories.
    >   2021-03-08 19:33:14,300 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) turned good: 1/1 local-dirs are good: /var/hadoop/temp/nm-local-dir; 1/1 log-dirs are good: /var/hadoop/logs/userlogs
    >   2021-03-08 19:33:17,363 INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1615204633239_0002_000001 (auth:SIMPLE)
    >   2021-03-08 19:33:17,368 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1615204633239_0002_01_000022 by user fedora
    >   2021-03-08 19:33:17,370 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       IP=10.10.2.213  OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_16152046332
    >   39_0002    CONTAINERID=container_1615204633239_0002_01_000022
    >   2021-03-08 19:33:17,370 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1615204633239_0002_01_000022 to application application_1615204633239_0002
    >   2021-03-08 19:33:17,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from NEW to LOCALIZING
    >   2021-03-08 19:33:17,372 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1615204633239_0002
    >   2021-03-08 19:33:17,373 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from LOCALIZING to SCHEDULED
    >   2021-03-08 19:33:17,373 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler: Starting container [container_1615204633239_0002_01_000022]
    >   2021-03-08 19:33:17,387 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from SCHEDULED to RUNNING
    >   2021-03-08 19:33:17,388 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1615204633239_0002_01_000022
    >   2021-03-08 19:33:17,390 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: launchContainer: [bash, /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000022/default_container_executor.sh]
    >   2021-03-08 19:33:18,306 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: container_1615204633239_0002_01_000022's ip = 10.10.1.196, and hostname = worker01
    >   2021-03-08 19:33:18,315 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Skipping monitoring container container_1615204633239_0002_01_000022 since CPU usage is not yet available.
    >   2021-03-08 19:35:14,298 WARN org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection: Directory /var/hadoop/temp/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories
    >   2021-03-08 19:35:14,299 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed: 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:35:14,300 ERROR org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Most of the disks failed. 1/1 local-dirs usable space is below configured utilization percentage/no more usable space [ /var/hadoop/temp/nm-local-dir : used space above threshold of 90.0% ] ;
    >   2021-03-08 19:35:16,193 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from RUNNING to KILLING
    >   2021-03-08 19:35:16,193 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Cleaning up container container_1615204633239_0002_01_000022
    >   2021-03-08 19:35:16,300 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1615204633239_0002_01_000022 is : 143
    >   2021-03-08 19:35:16,300 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher: Got exception while cleaning container container_1615204633239_0002_01_000022. Ignoring.
    >   2021-03-08 19:35:16,300 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from KILLING to CONTAINER_CLEANEDUP_AFTER_KILL
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/container_1615204633239_0002_01_000022
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=fedora       OPERATION=Container Finished - Killed   TARGET=ContainerImpl    RESULT=SUCCESS  APPID=application_1615204633239_0002    CONTAINERID=container_1615204633239_0002_01_000022
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1615204633239_0002_01_000022 transitioned from CONTAINER_CLEANEDUP_AFTER_KILL to DONE
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1615204633239_0002_01_000022 from application application_1615204633239_0002
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1615204633239_0002_01_000022
    >   2021-03-08 19:35:16,301 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1615204633239_0002
    >   2021-03-08 19:35:17,304 INFO org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1615204633239_0002_01_000022]
    >   2021-03-08 19:37:14,558 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:47:14,558 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 19:57:14,559 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:07:14,559 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:17:14,559 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   2021-03-08 20:27:14,559 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Cache Size Before Clean: 489628470, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0
    >   ....


# -----------------------------------------------------
# Check the available disc space.
#[fedora@worker01]

    df -h /var/hadoop/temp/nm-local-dir

    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdb         59G   54G  2.6G  96% /mnt/local/vdb


    du -h /var/hadoop/temp/

    >   ....
    >   ....
    >   139M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-61e99471-185a-45f0-9d55-258738003e28/3b
    >   139M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-61e99471-185a-45f0-9d55-258738003e28/3c
    >   139M    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-61e99471-185a-45f0-9d55-258738003e28/04
    >   5.3G    /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002/blockmgr-61e99471-185a-45f0-9d55-258738003e28
    >   53G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache/application_1615204633239_0002
    >   53G     /var/hadoop/temp/nm-local-dir/usercache/fedora/appcache
    >
    >   469M	/var/hadoop/temp/nm-local-dir/usercache/fedora/filecache
    >   54G     /var/hadoop/temp/nm-local-dir/usercache/fedora
    >   54G     /var/hadoop/temp/nm-local-dir/usercache
    >   54G     /var/hadoop/temp/nm-local-dir
    >   54G     /var/hadoop/temp/

