#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

# -----------------------------------------------------
# Create a container to work with.
# https://podman.readthedocs.io/en/latest/markdown/podman-run.1.html
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name ansibler \
        --hostname ansibler \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --env "ANSIBLE_CODE=/mnt/ansible" \
        --volume "${AGLAIS_CODE:?}/experiments/zrq/ansible:/mnt/ansible:ro,z" \
        atolmis/ansible-client:latest \
        bash


# -----------------------------------------------------
# Create our Ansible include vars file.
#[root@ansibler]

    cat > /tmp/ansible-vars.yml << EOF
buildtag:  'aglais-$(date '+%Y%m%d')'
cloudname: '${cloudname}'
clouduser: '${clouduser}'
EOF


# -----------------------------------------------------
# Run the scripts from the ansible directory.
#[root@ansibler]

    # TODO Make this the working directory in the container ?
    # --env ANSIBLE_CODE=/mnt/ansible

    cd "${ANSIBLE_CODE:?}"


# -----------------------------------------------------
# Run the initial part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-01.yml"

--START--
....
....
--END--


# -----------------------------------------------------
# Run the Hadoop part of our deplyment.
#[root@ansibler]

    ansible-playbook \
        --inventory "hosts.yml" \
        "combined-02.yml"

--START--
....
....
--END--


# -----------------------------------------------------
# Format the HDFS NameNode on master01.
#[root@ansibler]

    ssh master01 \
        '
        hdfs namenode -format
        '

--START--
2020-04-11 06:01:36,015 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = aglais-20200411-master01.novalocal/10.10.0.19
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.2.1
STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/....
STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
STARTUP_MSG:   java = 13.0.2
************************************************************/
2020-04-11 06:01:36,026 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-04-11 06:01:36,138 INFO namenode.NameNode: createNameNode [-format]
2020-04-11 06:01:36,479 INFO common.Util: Assuming 'file' scheme for path /var/local/hadoop/namenode/fsimage in configuration.
Formatting using clusterid: CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a
2020-04-11 06:01:36,479 INFO common.Util: Assuming 'file' scheme for path /var/local/hadoop/namenode/fsimage in configuration.
2020-04-11 06:01:36,515 INFO namenode.FSEditLog: Edit logging is async:true
2020-04-11 06:01:36,527 INFO namenode.FSNamesystem: KeyProvider: null
2020-04-11 06:01:36,528 INFO namenode.FSNamesystem: fsLock is fair: true
2020-04-11 06:01:36,528 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-04-11 06:01:36,558 INFO namenode.FSNamesystem: fsOwner             = fedora (auth:SIMPLE)
2020-04-11 06:01:36,559 INFO namenode.FSNamesystem: supergroup          = supergroup
2020-04-11 06:01:36,559 INFO namenode.FSNamesystem: isPermissionEnabled = true
2020-04-11 06:01:36,559 INFO namenode.FSNamesystem: HA Enabled: false
2020-04-11 06:01:36,602 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-11 06:01:36,612 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-11 06:01:36,612 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-11 06:01:36,617 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-11 06:01:36,617 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Apr 11 06:01:36
2020-04-11 06:01:36,618 INFO util.GSet: Computing capacity for map BlocksMap
2020-04-11 06:01:36,618 INFO util.GSet: VM type       = 64-bit
2020-04-11 06:01:36,620 INFO util.GSet: 2.0% max memory 1.5 GB = 29.8 MB
2020-04-11 06:01:36,620 INFO util.GSet: capacity      = 2^22 = 4194304 entries
2020-04-11 06:01:36,639 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
2020-04-11 06:01:36,639 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2020-04-11 06:01:36,646 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2020-04-11 06:01:36,646 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-11 06:01:36,646 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2020-04-11 06:01:36,646 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2020-04-11 06:01:36,646 INFO blockmanagement.BlockManager: defaultReplication         = 2
2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: maxReplication             = 512
2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: minReplication             = 1
2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2020-04-11 06:01:36,647 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-04-11 06:01:36,665 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2020-04-11 06:01:36,665 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2020-04-11 06:01:36,665 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2020-04-11 06:01:36,665 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
2020-04-11 06:01:36,676 INFO util.GSet: Computing capacity for map INodeMap
2020-04-11 06:01:36,676 INFO util.GSet: VM type       = 64-bit
2020-04-11 06:01:36,676 INFO util.GSet: 1.0% max memory 1.5 GB = 14.9 MB
2020-04-11 06:01:36,676 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2020-04-11 06:01:36,682 INFO namenode.FSDirectory: ACLs enabled? false
2020-04-11 06:01:36,682 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2020-04-11 06:01:36,682 INFO namenode.FSDirectory: XAttrs enabled? true
2020-04-11 06:01:36,682 INFO namenode.NameNode: Caching file names occurring more than 10 times
2020-04-11 06:01:36,685 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-11 06:01:36,687 INFO snapshot.SnapshotManager: SkipList is disabled
2020-04-11 06:01:36,690 INFO util.GSet: Computing capacity for map cachedBlocks
2020-04-11 06:01:36,690 INFO util.GSet: VM type       = 64-bit
2020-04-11 06:01:36,691 INFO util.GSet: 0.25% max memory 1.5 GB = 3.7 MB
2020-04-11 06:01:36,691 INFO util.GSet: capacity      = 2^19 = 524288 entries
2020-04-11 06:01:36,697 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-11 06:01:36,698 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-04-11 06:01:36,698 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-11 06:01:36,700 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2020-04-11 06:01:36,701 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-11 06:01:36,702 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2020-04-11 06:01:36,702 INFO util.GSet: VM type       = 64-bit
2020-04-11 06:01:36,702 INFO util.GSet: 0.029999999329447746% max memory 1.5 GB = 457.7 KB
2020-04-11 06:01:36,702 INFO util.GSet: capacity      = 2^16 = 65536 entries
2020-04-11 06:01:36,725 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1435812019-10.10.0.19-1586584896719
2020-04-11 06:01:36,755 INFO common.Storage: Storage directory /var/local/hadoop/namenode/fsimage has been successfully formatted.
2020-04-11 06:01:36,785 INFO namenode.FSImageFormatProtobuf: Saving image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 using no compression
2020-04-11 06:01:36,862 INFO namenode.FSImageFormatProtobuf: Image file /var/local/hadoop/namenode/fsimage/current/fsimage.ckpt_0000000000000000000 of size 401 bytes saved in 0 seconds .
2020-04-11 06:01:36,870 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2020-04-11 06:01:36,879 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2020-04-11 06:01:36,880 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at aglais-20200411-master01.novalocal/10.10.0.19
************************************************************/
--END--


# -----------------------------------------------------
# Start the HDFS services.
#[root@ansibler]

    ssh master01 \
        '
        start-dfs.sh
        '

--START--
Starting namenodes on [master01]
Starting datanodes
Starting secondary namenodes [aglais-20200411-master01.novalocal]
aglais-20200411-master01.novalocal: Warning: Permanently added 'aglais-20200411-master01.novalocal,fe80::f816:3eff:fe29:c013%eth0' (ECDSA) to the list of known hosts.
--END--

    # TODO - secondary namenode on the same host ?
    # TODO - Move the secondary namenode to another host ..


# -----------------------------------------------------
# Check the HDFS status.
#[root@ansibler]

    ssh master01 \
        '
        hdfs dfsadmin -report
        '

--START--
Configured Capacity: 4398046511104 (4 TB)
Present Capacity: 4389384241152 (3.99 TB)
DFS Remaining: 4389384224768 (3.99 TB)
DFS Used: 16384 (16 KB)
DFS Used%: 0.00%
Replicated Blocks:
	Under replicated blocks: 0
	Blocks with corrupt replicas: 0
	Missing blocks: 0
	Missing blocks (with replication factor 1): 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0
Erasure Coded Block Groups: 
	Low redundancy block groups: 0
	Block groups with corrupt internal blocks: 0
	Missing block groups: 0
	Low redundancy blocks with highest priority to recover: 0
	Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (4):

Name: 10.10.0.25:9866 (worker01)
Hostname: worker01
Decommission Status : Normal
Configured Capacity: 1099511627776 (1 TB)
DFS Used: 4096 (4 KB)
Non DFS Used: 17297408 (16.50 MB)
DFS Remaining: 1097346056192 (1021.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.80%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Sat Apr 11 06:06:03 UTC 2020
Last Block Report: Sat Apr 11 06:05:06 UTC 2020
Num of Blocks: 0


Name: 10.10.0.29:9866 (worker03)
Hostname: worker03
Decommission Status : Normal
Configured Capacity: 1099511627776 (1 TB)
DFS Used: 4096 (4 KB)
Non DFS Used: 17297408 (16.50 MB)
DFS Remaining: 1097346056192 (1021.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.80%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Sat Apr 11 06:06:03 UTC 2020
Last Block Report: Sat Apr 11 06:05:06 UTC 2020
Num of Blocks: 0


Name: 10.10.0.4:9866 (worker02)
Hostname: worker02
Decommission Status : Normal
Configured Capacity: 1099511627776 (1 TB)
DFS Used: 4096 (4 KB)
Non DFS Used: 17297408 (16.50 MB)
DFS Remaining: 1097346056192 (1021.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.80%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Sat Apr 11 06:06:03 UTC 2020
Last Block Report: Sat Apr 11 06:05:06 UTC 2020
Num of Blocks: 0


Name: 10.10.0.6:9866 (worker04)
Hostname: worker04
Decommission Status : Normal
Configured Capacity: 1099511627776 (1 TB)
DFS Used: 4096 (4 KB)
Non DFS Used: 17297408 (16.50 MB)
DFS Remaining: 1097346056192 (1021.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.80%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Sat Apr 11 06:06:03 UTC 2020
Last Block Report: Sat Apr 11 06:05:06 UTC 2020
Num of Blocks: 0
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Tail the logs (separate terminals).
#[root@ansibler]


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh master01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-namenode-$(hostname).log
            '

--START--
....
2020-04-11 06:05:06,146 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN 93ef822c-2028-4b31-9fbc-c392bcc62576 (10.10.0.6:9866).
2020-04-11 06:05:06,182 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-25b0108d-414f-40b3-9a87-0262c3850155 for DN 10.10.0.6:9866
2020-04-11 06:05:06,182 INFO BlockStateChange: BLOCK* processReport 0xab9a08a357b20fc4: Processing first storage report for DS-0b65e482-6c56-4270-8925-395ccd629c2a from datanode 68a1bdc5-5c07-4e4e-8714-6d8545a70895
2020-04-11 06:05:06,187 INFO BlockStateChange: BLOCK* processReport 0xab9a08a357b20fc4: from storage DS-0b65e482-6c56-4270-8925-395ccd629c2a node DatanodeRegistration(10.10.0.29:9866, datanodeUuid=68a1bdc5-5c07-4e4e-8714-6d8545a70895, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a;nsid=764607163;c=1586584896719), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-04-11 06:05:06,187 INFO BlockStateChange: BLOCK* processReport 0x392e2bbdbd118d02: Processing first storage report for DS-2645a5fe-b95e-42a5-a7ef-8a1109b58f23 from datanode a6ec9264-1050-480e-959e-9234570ba488
2020-04-11 06:05:06,188 INFO BlockStateChange: BLOCK* processReport 0x392e2bbdbd118d02: from storage DS-2645a5fe-b95e-42a5-a7ef-8a1109b58f23 node DatanodeRegistration(10.10.0.25:9866, datanodeUuid=a6ec9264-1050-480e-959e-9234570ba488, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a;nsid=764607163;c=1586584896719), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-11 06:05:06,188 INFO BlockStateChange: BLOCK* processReport 0x838942d2c2e97e01: Processing first storage report for DS-26a8671f-c859-4e62-9a2d-e6b52ad78507 from datanode 49a657ee-27c9-49ab-9af6-6b05b8fce8ea
2020-04-11 06:05:06,188 INFO BlockStateChange: BLOCK* processReport 0x838942d2c2e97e01: from storage DS-26a8671f-c859-4e62-9a2d-e6b52ad78507 node DatanodeRegistration(10.10.0.4:9866, datanodeUuid=49a657ee-27c9-49ab-9af6-6b05b8fce8ea, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a;nsid=764607163;c=1586584896719), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-11 06:05:06,196 INFO BlockStateChange: BLOCK* processReport 0xb73bbd2a3d5884e7: Processing first storage report for DS-25b0108d-414f-40b3-9a87-0262c3850155 from datanode 93ef822c-2028-4b31-9fbc-c392bcc62576
2020-04-11 06:05:06,196 INFO BlockStateChange: BLOCK* processReport 0xb73bbd2a3d5884e7: from storage DS-25b0108d-414f-40b3-9a87-0262c3850155 node DatanodeRegistration(10.10.0.6:9866, datanodeUuid=93ef822c-2028-4b31-9fbc-c392bcc62576, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-1894d4d5-1949-4c21-a394-559ccc2c5f0a;nsid=764607163;c=1586584896719), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
....
--END--


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker01 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-datanode-$(hostname).log
            '

--START--
....
2020-04-11 06:05:05,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1435812019-10.10.0.19-1586584896719: 3ms
2020-04-11 06:05:05,996 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1435812019-10.10.0.19-1586584896719 on volume /data-01/hdfs/data
2020-04-11 06:05:05,997 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-2645a5fe-b95e-42a5-a7ef-8a1109b58f23): finished scanning block pool BP-1435812019-10.10.0.19-1586584896719
2020-04-11 06:05:06,018 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/11/20, 9:35 AM with interval of 21600000ms
2020-04-11 06:05:06,019 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-2645a5fe-b95e-42a5-a7ef-8a1109b58f23): no suitable block pools found to scan.  Waiting 1814399977 ms.
2020-04-11 06:05:06,023 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1435812019-10.10.0.19-1586584896719 (Datanode Uuid a6ec9264-1050-480e-959e-9234570ba488) service to master01/10.10.0.19:9000 beginning handshake with NN
2020-04-11 06:05:06,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1435812019-10.10.0.19-1586584896719 (Datanode Uuid a6ec9264-1050-480e-959e-9234570ba488) service to master01/10.10.0.19:9000 successfully registered with NN
2020-04-11 06:05:06,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master01/10.10.0.19:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-11 06:05:06,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x392e2bbdbd118d02,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 71 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-11 06:05:06,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1435812019-10.10.0.19-1586584896719
....
--END--


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker02 \
            '
            tail -f /var/local/hadoop/logs/hadoop-fedora-datanode-$(hostname).log
            '

--START--
....
2020-04-11 06:05:06,005 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1435812019-10.10.0.19-1586584896719: 6ms
2020-04-11 06:05:06,008 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1435812019-10.10.0.19-1586584896719 on volume /data-01/hdfs/data
2020-04-11 06:05:06,009 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-26a8671f-c859-4e62-9a2d-e6b52ad78507): finished scanning block pool BP-1435812019-10.10.0.19-1586584896719
2020-04-11 06:05:06,038 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/11/20, 8:32 AM with interval of 21600000ms
2020-04-11 06:05:06,039 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/data-01/hdfs/data, DS-26a8671f-c859-4e62-9a2d-e6b52ad78507): no suitable block pools found to scan.  Waiting 1814399967 ms.
2020-04-11 06:05:06,044 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1435812019-10.10.0.19-1586584896719 (Datanode Uuid 49a657ee-27c9-49ab-9af6-6b05b8fce8ea) service to master01/10.10.0.19:9000 beginning handshake with NN
2020-04-11 06:05:06,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1435812019-10.10.0.19-1586584896719 (Datanode Uuid 49a657ee-27c9-49ab-9af6-6b05b8fce8ea) service to master01/10.10.0.19:9000 successfully registered with NN
2020-04-11 06:05:06,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master01/10.10.0.19:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-11 06:05:06,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x838942d2c2e97e01,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 74 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-11 06:05:06,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1435812019-10.10.0.19-1586584896719
....
--END--

    #
    # Test the HDFS system ...
    #




# -----------------------------------------------------
# Start the YARN services.
#[root@ansibler]

    ssh master01 \
        '
        start-yarn.sh
        '

--START--
....
....
--END--


# -----------------------------------------------------
# -----------------------------------------------------
# Tail the logs (separate terminals).
#[root@ansibler]



    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh master01 \
            '
            ls -al /var/local/hadoop/logs
            '

--START--
....
....
--END--


    podman exec -it $(
        podman ps --filter 'name=ansibler' --format "{{.ID}}"
        ) \
        ssh worker01 \
            '
            ls -al /var/local/hadoop/logs
            '

--START--
....
....
--END--

