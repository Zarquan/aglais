#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#

    Notes on how we got where we are.
    Source material for the status report in September.

    Kubernetes is the new OS.
    Moving towards defining everything interms of Kubernetes components.

    It is a good abstraction layer for cloud computing components

        generic abstraction of components
            storage space
            network access
            computing nodes

    Openstack is too low level in some places, too insulated in others,

    Difficult to say "run this" in Openstack.

    Openstack is modelled around virtual machines
        including (virtual) PCI and ISA bus,
        network cards and discs,
        BIOS and operating system
        components that mimic physical world
    Kubernetes is modelled around containers
        bubbles of namespace that float in the cloud
        thin layer with interfaces to abstract things
        like storage and network

    Openstack is a continuation of the virtual machine based
    cloud compute model from early 2000's

             Linode in 2003 (UML in 2003, Xen in .., KVM in ..)
             Amazon web services in 2003
             became EC2 in 2006
             2008 GoogleASppEngine
             2010 Microsoft Azure
             2010 RackSpace and NASA Openstack

    Gradually moving to higerh levels of abstraction

    Mid 2000's people begin to work on automation

        Puppet, Chef and then Ansible

    Still basedon modelling real world hardware though
        A computer with a dics connected to a network
        An administrator logs in via ssh to configure it

    In some places this reveals too much detail.
    In Openstack a newbie user has to create a subnet and a router before they can do anything.

    Commercial providers have always been way ahead in this, but up to recently
    they have been (virtual) machine based too.

    In other places, too little detail. No access to performance metrics
    on the underlying hardware. You have to imagine what is actually happening
    at the hardware level because you can't see it.

    All based on the 2000's model of virtualization,
    compute nodes connected to a SAN.

        blades        -- high speed network --> SAN
    high density compute                    high density storage

    High density anything causes thermal problems.
    Dissipating heat is the big issue.

    ---------------

    Meanwhile

    Distributed systems are working on a different model

        MapReduce (Google)
        Hadoop (Yahoo)
        Spark ()
        Kafka (Uber, LinkedIn)
        Cassandra

    All designed to run on large array of *cheap* hardware.

        [cheap compute and disc] x 1,000's

    New buzz word

    Hyperconverged
    DirectAttachedStorage

    means going back to machines with discs
    undoing the SAN model

    move the code to the data
    or
    store the data in the computer

        Hadoop
        Spark
        Kafka
        Cassandra
        on top of Openstack

    software designed for DAS running on
    system designed around SAN

    quick and easy deployment will work
    but not making best use of the resources

    distributed applications like this already have
    partitioning and replication built in

    running them on Openstack and Cinder replicates
    the replication and partitioning
    creating a tangled mix

    ---------------

    Moving away from SAN based model .. to a SAN based model.

    Cinder hides the SAN.
    Makes it look like we have a disc attached to a machine, when in fact the disc is
    on a different machine in a separate rack.

    If we embrace the SAN, then we avoid an un necessary layer of indirection.

    Zeppelin, Hadoop/Yarn and Spark can all work with data in network filesystems.

    Openstack provides access to shared data in an object store API, Switf,
    or a remote files system API, Manila.

    Swift has its own web service API, or it can mimic the Amazon S3 API.
    We experimented with the Swift API, but found problems with the Java client libraries.
    Out of date versions, conflicting doicumentation etc..

        Several gotchas along the way.
        Abandoned in favor of the S3 API.

    We experimented with the Amazon S3 API.
    Support for the Java client was better, but almost all of the documentation
    about S3 in Spark is for using real S3 services hosted on Amazon cloud.
    Limited documentation for accessins a Swift based S3 service from Spark,
    and whart was there claimed it all just works, when our experience
    was that it all just didn't.

        Several gotchas along the way.
        Abandoned in favor of the CephFS API.

    The Amazon S3 API is worth persuing later because it is common API
    available on all of the commercial cloud providers.
    Being able to use this would make our deployemnt portable to commercial providers.

    Openstack Manila
    ....
    Provides NFS and CephFS network filesystems.
    Cambridge cloud only supports CephFS.

    Kubernetes plug-in for Openstack Manila provides access to
    Openstack CephFS shares as Kubernetes PersistentVolumes.

        Several gotchas along the way.

    Working deployment of gara DR2 on Openstack CephFS share
    mounted in Spark Pods as a Kubernetes PersistentVolume.

    Note this is not the same as implementing NFS or CephFS using virtual machines
    with Cinder volumes.
    This is direct connection to the underlying Ceph storage system
    that provides all the storage for the Cambridge cloud.



    ---------------

    Meanwhile

    If you want to know where the IT industry is going,
    follow the big players

    social networking systems have the biggest data

    Twitter, Facebook, Uber, LinkedIn
    Google, Microsoft
    data size and rates larger than
    all the science cases

    Twitter, Facebook, Uber, LinkedIn
    Google, Microsoft
    research funding larger than
    all the science cases

    Twitter, Facebook, Uber, LinkedIn
    Google, Microsoft
    commercial pressures tighter than
    all the science cases

    huge pressure to do more with less

    underneath, they are all using Linux on x86
    ontop, they are all using Kubernetes to abstract the details away

    next big thing will be ML 'applications' on top of abstracted compute
    on top of implies building a stack
    more abstract will sending your model to run in a bubble
    on someone else's stack

    ---------------

    Back to the task in hand

    Gradual evolution of Spark follows the evolution of cloud computing

    Spark originally designed to run with Haddoop on top of
    a cluster of physical machines

    Used the Hadoop scheduler, Yarn, to manage jobs.

    However, Spark designed to be agnostic about which scheduler.
    So can use different resource amanagers,
        Yarm,
        Mesos,
        xxx
     and now
        Kubernetes

    Spark client
        submits job to
    Spark driver
        driver breaks it into steps
        and sends steps to
    Spark executor(s)

    Cluster mode - master node launches driver in separate process/node
    which then spawns executor processes as needed.

    Client mode, in-process driver spawns executor processes as needed.

    Openstack based deployment assigns virtual machines for each component

        Zeppelin virtual machine
            spark submit sends job to master
        Spark master virtual machine
            uses Yarn to distribute tasks to Spark workers
        Spark worker virtual machine(s)
            execute Yarn tasks on demand

    All the virtual machines are static long lived resources.
    Each machine is configured manually, or automated using Ansible.

    Experimented with Kubernetes deploymetns that use the same model.
    Using Helm charts to create Pods that replicate the same components.

        Zeppelin Pod
            spark submit sends job to master
        Spark master Pod
            uses Yarn to distribute tasks to Spark workers
        Spark worker Pods
            execute Yarn tasks on demand

    Some of the Pods like Zeppelin and Spark master are long lived resources,
    others, like Spark workers, can be created on demand.

    Latest change is to treat the whole Spark cluster as an on demand resource.

    Zeppelin server
        Web server, provides the UI and container for notebooks
    Notebook cell
        Unit of software, sent toi an interpreter
    Zeppelin interpreter
        Executes code in a notebook cell
        Separate interpreters for each language
            Python, Java, Spark etc.
        Separate interpreters for each notebook <-- new
            Zeppelin server calls Kubernetes API to launch interpreters on demand.

        Spark interpreter is a special case of generic interpreter
            Spark interpreter contains Spark client.
            Submits Spark jobs in client mode.
            Spark interpreter *is* a Spark driver.
            Spark driver calls Kubernetes API to launch Spark executors on demand.

        Spark executors are short lived Pods launched and released on demand.

    We are following this trend as the software evolves.

    We have Spark/Yarn based deployments that work.
    We have Spark/Yarn on Kubernetes deployments that work.
    We are working on direct Spark on Kubernetes deployments.

    Zeppelin Pods launching Spark on demand as a cluster of Pods is the way things are evolving.
    This is what will be in Zeppelin 0.9.0 release
        https://zeppelin.apache.org/download.html
    Currently (Aug 2020) available as 0.9.0-preview2 (released July 2020)

    Whether we will have everything in place for December launch is up for debate.
    If we don't do it in Dec, we will have to re-do it in early 2021 to make use of
    features released in Zeppelin and Spark in 2021.

    Keeping Spark/Yarn based deployments active as a backup.









Origins of Ansible
https://www.ansible.com/blog/2013/12/08/the-origins-of-ansible

https://en.wikipedia.org/wiki/Ansible_(software)

https://en.wikipedia.org/wiki/Linode

https://en.wikipedia.org/wiki/Cloud_computing

https://en.wikipedia.org/wiki/OpenStack
































